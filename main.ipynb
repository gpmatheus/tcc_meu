{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33152, 201, 201, 3)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('train.h5', mode='r') as f:\n",
    "    print(f['matrix'].shape)\n",
    "    data_len = f['matrix'].shape[0]\n",
    "\n",
    "with h5py.File('valid.h5', mode='r') as f:\n",
    "    valid_data_len = f['matrix'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "load_batch = 1024\n",
    "batch = 4\n",
    "iter_train = data_len // batch\n",
    "iter_valid = valid_data_len // batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(file):\n",
    "    with h5py.File(file, mode='r') as src:\n",
    "        data_len = src['matrix'].shape[0]\n",
    "        batch_indexes = np.array(list(range(int(np.ceil(data_len / load_batch)))))\n",
    "        for e in range(epochs):\n",
    "            np.random.shuffle(batch_indexes)\n",
    "            for i in batch_indexes:\n",
    "                slc = slice(i, i + load_batch if i + load_batch < data_len else data_len)\n",
    "                with tf.device('/GPU:0'):\n",
    "                    # seleciona somente canais de infravermelho e microondas\n",
    "                    images = src['matrix'][slc, :, :, [0, 2]]\n",
    "                    # images = src['matrix'][slc]\n",
    "                    images = np.nan_to_num(images, copy=False)\n",
    "                    images[images > 1000] = 0.0\n",
    "                    avgs = tf.math.reduce_mean(images, axis=[0, 1, 2])\n",
    "                    stds = tf.math.reduce_std(images, axis=[0, 1, 2])\n",
    "                    norm = (images - avgs) / stds\n",
    "                    img_height = img_width = 201\n",
    "                    img_crop_w = 64\n",
    "                    st1 = (img_height // 2 - img_crop_w // 2)\n",
    "                    height_crop = slice(st1, st1 + img_crop_w)\n",
    "                    st2 = (img_width // 2 - img_crop_w // 2)\n",
    "                    width_crop = slice(st2, st2 + img_crop_w)\n",
    "                    norm = norm[:, height_crop, width_crop, :]\n",
    "                info = src['info'][slc]\n",
    "                info = tf.convert_to_tensor(info)\n",
    "                for j in range(norm.shape[0]):\n",
    "                    yield norm[j], tf.expand_dims(info[j], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator('train.h5')\n",
    "valid_generator = generator('valid.h5')\n",
    "test_generator = generator('test.h5')\n",
    "\n",
    "img_w = 64\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(img_w, img_w, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(1,), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: train_generator,\n",
    "    output_signature=output_signature\n",
    ").batch(1)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: valid_generator,\n",
    "    output_signature=output_signature\n",
    ").batch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: test_generator,\n",
    "    output_signature=output_signature\n",
    ").batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_model(input_shape, strides=(2, 2)):\n",
    "    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "    reg = keras.regularizers.L2(1e-5)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "    model.add(keras.layers.Conv2D(16, (4, 4), strides=strides, activation='relu', kernel_initializer=initializer, bias_initializer=initializer, kernel_regularizer=reg, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, bias_initializer=initializer, kernel_regularizer=reg, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, bias_initializer=initializer, kernel_regularizer=reg, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, bias_initializer=initializer, kernel_regularizer=reg, bias_regularizer=reg))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(1, activation='linear', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=.0005), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 31, 31, 16)        784       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 15, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,457\n",
      "Trainable params: 409,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model((64, 64, 3),)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8288/8288 [==============================] - 203s 23ms/step - loss: 494.3575 - mse: 494.3570 - val_loss: 344.8959 - val_mse: 344.8951\n",
      "Epoch 2/500\n",
      "8288/8288 [==============================] - 146s 18ms/step - loss: 379.9904 - mse: 379.9890 - val_loss: 319.8610 - val_mse: 319.8595\n",
      "Epoch 3/500\n",
      "8288/8288 [==============================] - 145s 17ms/step - loss: 293.6749 - mse: 293.6725 - val_loss: 284.1541 - val_mse: 284.1508\n",
      "Epoch 4/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 225.0889 - mse: 225.0845 - val_loss: 292.8994 - val_mse: 292.8941\n",
      "Epoch 5/500\n",
      "8288/8288 [==============================] - 145s 18ms/step - loss: 170.3094 - mse: 170.3029 - val_loss: 255.8173 - val_mse: 255.8098\n",
      "Epoch 6/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 113.8774 - mse: 113.8684 - val_loss: 330.1697 - val_mse: 330.1596\n",
      "Epoch 7/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 68.4862 - mse: 68.4747 - val_loss: 316.5393 - val_mse: 316.5269\n",
      "Epoch 8/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 53.5811 - mse: 53.5675 - val_loss: 325.3520 - val_mse: 325.3375\n",
      "Epoch 9/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 42.3628 - mse: 42.3475 - val_loss: 295.9733 - val_mse: 295.9567\n",
      "Epoch 10/500\n",
      "8288/8288 [==============================] - 134s 16ms/step - loss: 35.1516 - mse: 35.1340 - val_loss: 324.8643 - val_mse: 324.8457\n",
      "Epoch 11/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 33.1652 - mse: 33.1458 - val_loss: 379.2066 - val_mse: 379.1867\n",
      "Epoch 12/500\n",
      "8288/8288 [==============================] - 130s 16ms/step - loss: 27.0755 - mse: 27.0544 - val_loss: 300.7734 - val_mse: 300.7513\n",
      "Epoch 13/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 23.6199 - mse: 23.5971 - val_loss: 266.6773 - val_mse: 266.6540\n",
      "Epoch 14/500\n",
      "8288/8288 [==============================] - 134s 16ms/step - loss: 22.6940 - mse: 22.6697 - val_loss: 313.5585 - val_mse: 313.5338\n",
      "Epoch 15/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 22.0339 - mse: 22.0083 - val_loss: 287.4943 - val_mse: 287.4682\n",
      "Epoch 16/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 20.6166 - mse: 20.5898 - val_loss: 506.8711 - val_mse: 506.8431\n",
      "Epoch 17/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 17.0850 - mse: 17.0572 - val_loss: 263.8350 - val_mse: 263.8065\n",
      "Epoch 18/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 18.0937 - mse: 18.0649 - val_loss: 271.1733 - val_mse: 271.1439\n",
      "Epoch 19/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 14.2403 - mse: 14.2105 - val_loss: 310.3637 - val_mse: 310.3336\n",
      "Epoch 20/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 17.5555 - mse: 17.5248 - val_loss: 260.8023 - val_mse: 260.7711\n",
      "Epoch 21/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 13.7465 - mse: 13.7148 - val_loss: 253.3850 - val_mse: 253.3530\n",
      "Epoch 22/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 18.9163 - mse: 18.8835 - val_loss: 253.3597 - val_mse: 253.3265\n",
      "Epoch 23/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 13.0999 - mse: 13.0662 - val_loss: 256.8515 - val_mse: 256.8175\n",
      "Epoch 24/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 12.5914 - mse: 12.5568 - val_loss: 253.0824 - val_mse: 253.0478\n",
      "Epoch 25/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 12.8078 - mse: 12.7723 - val_loss: 245.3536 - val_mse: 245.3174\n",
      "Epoch 26/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 11.3606 - mse: 11.3242 - val_loss: 275.7734 - val_mse: 275.7365\n",
      "Epoch 27/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 11.9202 - mse: 11.8832 - val_loss: 270.4798 - val_mse: 270.4424\n",
      "Epoch 28/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 13.3664 - mse: 13.3282 - val_loss: 286.2954 - val_mse: 286.2570\n",
      "Epoch 29/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 11.7111 - mse: 11.6718 - val_loss: 289.0295 - val_mse: 288.9893\n",
      "Epoch 30/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 10.6082 - mse: 10.5680 - val_loss: 262.0237 - val_mse: 261.9830\n",
      "Epoch 31/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 16.0431 - mse: 16.0020 - val_loss: 246.6786 - val_mse: 246.6366\n",
      "Epoch 32/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 11.8003 - mse: 11.7579 - val_loss: 241.4682 - val_mse: 241.4252\n",
      "Epoch 33/500\n",
      "8288/8288 [==============================] - 140s 16ms/step - loss: 10.6721 - mse: 10.6284 - val_loss: 251.0105 - val_mse: 250.9660\n",
      "Epoch 34/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 8.2904 - mse: 8.2456 - val_loss: 261.3779 - val_mse: 261.3329\n",
      "Epoch 35/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 10.0128 - mse: 9.9672 - val_loss: 255.6103 - val_mse: 255.5643\n",
      "Epoch 36/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 8.4953 - mse: 8.4486 - val_loss: 233.4062 - val_mse: 233.3587\n",
      "Epoch 37/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 9.4706 - mse: 9.4229 - val_loss: 244.3090 - val_mse: 244.2607\n",
      "Epoch 38/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 7.8840 - mse: 7.8354 - val_loss: 235.4339 - val_mse: 235.3847\n",
      "Epoch 39/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 8.6744 - mse: 8.6251 - val_loss: 258.2692 - val_mse: 258.2193\n",
      "Epoch 40/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 8.8828 - mse: 8.8326 - val_loss: 237.4337 - val_mse: 237.3832\n",
      "Epoch 41/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 8.4807 - mse: 8.4298 - val_loss: 240.1837 - val_mse: 240.1325\n",
      "Epoch 42/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 7.8960 - mse: 7.8445 - val_loss: 242.8170 - val_mse: 242.7650\n",
      "Epoch 43/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 7.2097 - mse: 7.1576 - val_loss: 249.2367 - val_mse: 249.1843\n",
      "Epoch 44/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 7.0706 - mse: 7.0180 - val_loss: 242.4570 - val_mse: 242.4045\n",
      "Epoch 45/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 6.7848 - mse: 6.7316 - val_loss: 242.3138 - val_mse: 242.2599\n",
      "Epoch 46/500\n",
      "8288/8288 [==============================] - 157s 19ms/step - loss: 7.0760 - mse: 7.0222 - val_loss: 252.8602 - val_mse: 252.8060\n",
      "Epoch 47/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 6.7148 - mse: 6.6603 - val_loss: 250.1125 - val_mse: 250.0576\n",
      "Epoch 48/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 6.8349 - mse: 6.7799 - val_loss: 247.2073 - val_mse: 247.1521\n",
      "Epoch 49/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 6.7045 - mse: 6.6487 - val_loss: 243.9029 - val_mse: 243.8466\n",
      "Epoch 50/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 6.3968 - mse: 6.3405 - val_loss: 244.0913 - val_mse: 244.0345\n",
      "Epoch 51/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.0650 - mse: 6.0080 - val_loss: 246.2846 - val_mse: 246.2274\n",
      "Epoch 52/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 16.2665 - mse: 16.2091 - val_loss: 246.4858 - val_mse: 246.4285\n",
      "Epoch 53/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.8028 - mse: 6.7450 - val_loss: 235.6223 - val_mse: 235.5642\n",
      "Epoch 54/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 7.4166 - mse: 7.3583 - val_loss: 243.0429 - val_mse: 242.9847\n",
      "Epoch 55/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.4254 - mse: 5.3666 - val_loss: 235.5591 - val_mse: 235.5000\n",
      "Epoch 56/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.2425 - mse: 6.1828 - val_loss: 245.0868 - val_mse: 245.0267\n",
      "Epoch 57/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 6.4309 - mse: 6.3703 - val_loss: 247.5917 - val_mse: 247.5310\n",
      "Epoch 58/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 6.5526 - mse: 6.4911 - val_loss: 246.0136 - val_mse: 245.9515\n",
      "Epoch 59/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.3637 - mse: 6.3012 - val_loss: 238.6561 - val_mse: 238.5933\n",
      "Epoch 60/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.1551 - mse: 6.0920 - val_loss: 291.3954 - val_mse: 291.3320\n",
      "Epoch 61/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.6563 - mse: 6.5924 - val_loss: 240.3121 - val_mse: 240.2476\n",
      "Epoch 62/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 6.1045 - mse: 6.0396 - val_loss: 255.8895 - val_mse: 255.8242\n",
      "Epoch 63/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.3728 - mse: 6.3071 - val_loss: 244.8431 - val_mse: 244.7772\n",
      "Epoch 64/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.4621 - mse: 6.3956 - val_loss: 236.2152 - val_mse: 236.1485\n",
      "Epoch 65/500\n",
      "8288/8288 [==============================] - 140s 16ms/step - loss: 6.4621 - mse: 6.3948 - val_loss: 239.9587 - val_mse: 239.8912\n",
      "Epoch 66/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 5.7980 - mse: 5.7299 - val_loss: 252.8469 - val_mse: 252.7782\n",
      "Epoch 67/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.2001 - mse: 6.1312 - val_loss: 234.5285 - val_mse: 234.4593\n",
      "Epoch 68/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.1572 - mse: 6.0875 - val_loss: 248.6477 - val_mse: 248.5777\n",
      "Epoch 69/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 6.1925 - mse: 6.1218 - val_loss: 248.2141 - val_mse: 248.1428\n",
      "Epoch 70/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 6.0750 - mse: 6.0033 - val_loss: 281.1119 - val_mse: 281.0405\n",
      "Epoch 71/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 5.2503 - mse: 5.1780 - val_loss: 243.0899 - val_mse: 243.0170\n",
      "Epoch 72/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 4.8510 - mse: 4.7781 - val_loss: 236.2696 - val_mse: 236.1961\n",
      "Epoch 73/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 5.8716 - mse: 5.7982 - val_loss: 256.6638 - val_mse: 256.5896\n",
      "Epoch 74/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.2858 - mse: 5.2117 - val_loss: 238.4659 - val_mse: 238.3915\n",
      "Epoch 75/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 8.6307 - mse: 8.5561 - val_loss: 246.9556 - val_mse: 246.8807\n",
      "Epoch 76/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.8273 - mse: 4.7521 - val_loss: 239.3744 - val_mse: 239.2990\n",
      "Epoch 77/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 5.6829 - mse: 5.6069 - val_loss: 255.1292 - val_mse: 255.0523\n",
      "Epoch 78/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.3213 - mse: 5.2447 - val_loss: 227.4405 - val_mse: 227.3640\n",
      "Epoch 79/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 5.0169 - mse: 4.9398 - val_loss: 259.6951 - val_mse: 259.6175\n",
      "Epoch 80/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.6011 - mse: 5.5233 - val_loss: 239.3053 - val_mse: 239.2267\n",
      "Epoch 81/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 8.0233 - mse: 7.9449 - val_loss: 282.8132 - val_mse: 282.7343\n",
      "Epoch 82/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.8907 - mse: 3.8116 - val_loss: 229.9433 - val_mse: 229.8637\n",
      "Epoch 83/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.9195 - mse: 4.8400 - val_loss: 256.7825 - val_mse: 256.7028\n",
      "Epoch 84/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.0149 - mse: 4.9348 - val_loss: 232.8921 - val_mse: 232.8125\n",
      "Epoch 85/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.8981 - mse: 4.8176 - val_loss: 256.8086 - val_mse: 256.7275\n",
      "Epoch 86/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 4.7618 - mse: 4.6808 - val_loss: 232.5382 - val_mse: 232.4568\n",
      "Epoch 87/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.4858 - mse: 4.4043 - val_loss: 239.4679 - val_mse: 239.3862\n",
      "Epoch 88/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.8508 - mse: 4.7689 - val_loss: 253.9005 - val_mse: 253.8185\n",
      "Epoch 89/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.5519 - mse: 4.4694 - val_loss: 238.8835 - val_mse: 238.8007\n",
      "Epoch 90/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.8573 - mse: 4.7745 - val_loss: 224.7582 - val_mse: 224.6750\n",
      "Epoch 91/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.4952 - mse: 4.4119 - val_loss: 255.5670 - val_mse: 255.4828\n",
      "Epoch 92/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.1801 - mse: 4.0963 - val_loss: 250.7779 - val_mse: 250.6942\n",
      "Epoch 93/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.6039 - mse: 4.5200 - val_loss: 254.6692 - val_mse: 254.5849\n",
      "Epoch 94/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.1700 - mse: 5.0857 - val_loss: 234.0469 - val_mse: 233.9620\n",
      "Epoch 95/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.3959 - mse: 4.3110 - val_loss: 232.5022 - val_mse: 232.4171\n",
      "Epoch 96/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 5.1905 - mse: 5.1051 - val_loss: 255.6577 - val_mse: 255.5721\n",
      "Epoch 97/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 5.3301 - mse: 5.2441 - val_loss: 244.3041 - val_mse: 244.2175\n",
      "Epoch 98/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.4811 - mse: 4.3944 - val_loss: 233.2910 - val_mse: 233.2040\n",
      "Epoch 99/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.4024 - mse: 5.3149 - val_loss: 235.0923 - val_mse: 235.0046\n",
      "Epoch 100/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 5.9410 - mse: 5.8531 - val_loss: 259.7090 - val_mse: 259.6205\n",
      "Epoch 101/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.2910 - mse: 5.2024 - val_loss: 256.9698 - val_mse: 256.8810\n",
      "Epoch 102/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.0615 - mse: 4.9722 - val_loss: 266.8638 - val_mse: 266.7744\n",
      "Epoch 103/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 5.1903 - mse: 5.1003 - val_loss: 244.8324 - val_mse: 244.7418\n",
      "Epoch 104/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 5.0859 - mse: 4.9953 - val_loss: 246.9057 - val_mse: 246.8148\n",
      "Epoch 105/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 4.5437 - mse: 4.4526 - val_loss: 226.6590 - val_mse: 226.5674\n",
      "Epoch 106/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.4303 - mse: 4.3387 - val_loss: 243.2900 - val_mse: 243.1978\n",
      "Epoch 107/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 5.1810 - mse: 5.0890 - val_loss: 243.4610 - val_mse: 243.3686\n",
      "Epoch 108/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.7639 - mse: 4.6712 - val_loss: 243.7432 - val_mse: 243.6503\n",
      "Epoch 109/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.4539 - mse: 4.3607 - val_loss: 231.2100 - val_mse: 231.1167\n",
      "Epoch 110/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.5694 - mse: 4.4757 - val_loss: 241.1654 - val_mse: 241.0714\n",
      "Epoch 111/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.0025 - mse: 3.9082 - val_loss: 231.9244 - val_mse: 231.8297\n",
      "Epoch 112/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 4.0384 - mse: 3.9435 - val_loss: 267.2274 - val_mse: 267.1320\n",
      "Epoch 113/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.6351 - mse: 4.5395 - val_loss: 291.4921 - val_mse: 291.3960\n",
      "Epoch 114/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 4.0862 - mse: 3.9899 - val_loss: 232.8800 - val_mse: 232.7837\n",
      "Epoch 115/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.0424 - mse: 3.9457 - val_loss: 243.8078 - val_mse: 243.7107\n",
      "Epoch 116/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.1652 - mse: 4.0681 - val_loss: 241.5273 - val_mse: 241.4300\n",
      "Epoch 117/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 4.2403 - mse: 4.1427 - val_loss: 241.4341 - val_mse: 241.3363\n",
      "Epoch 118/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 3.8942 - mse: 3.7960 - val_loss: 240.1463 - val_mse: 240.0480\n",
      "Epoch 119/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.7831 - mse: 3.6845 - val_loss: 240.1566 - val_mse: 240.0578\n",
      "Epoch 120/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.8027 - mse: 3.7037 - val_loss: 246.4193 - val_mse: 246.3196\n",
      "Epoch 121/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.0268 - mse: 3.9272 - val_loss: 240.0543 - val_mse: 239.9544\n",
      "Epoch 122/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.2301 - mse: 4.1300 - val_loss: 255.9250 - val_mse: 255.8248\n",
      "Epoch 123/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.9256 - mse: 3.8249 - val_loss: 250.3865 - val_mse: 250.2857\n",
      "Epoch 124/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.1767 - mse: 4.0755 - val_loss: 231.2948 - val_mse: 231.1937\n",
      "Epoch 125/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.9967 - mse: 3.8949 - val_loss: 258.3639 - val_mse: 258.2621\n",
      "Epoch 126/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.1886 - mse: 4.0863 - val_loss: 243.5249 - val_mse: 243.4225\n",
      "Epoch 127/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.7278 - mse: 3.6251 - val_loss: 238.3437 - val_mse: 238.2407\n",
      "Epoch 128/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.7409 - mse: 3.6378 - val_loss: 227.1959 - val_mse: 227.0928\n",
      "Epoch 129/500\n",
      "8288/8288 [==============================] - 140s 16ms/step - loss: 3.7670 - mse: 3.6637 - val_loss: 254.4912 - val_mse: 254.3873\n",
      "Epoch 130/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.0496 - mse: 3.9458 - val_loss: 232.9504 - val_mse: 232.8463\n",
      "Epoch 131/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 4.1172 - mse: 4.0130 - val_loss: 261.2331 - val_mse: 261.1283\n",
      "Epoch 132/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.7951 - mse: 3.6904 - val_loss: 231.5417 - val_mse: 231.4366\n",
      "Epoch 133/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.5920 - mse: 3.4868 - val_loss: 245.2970 - val_mse: 245.1917\n",
      "Epoch 134/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.5979 - mse: 3.4923 - val_loss: 243.4677 - val_mse: 243.3622\n",
      "Epoch 135/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.7703 - mse: 3.6644 - val_loss: 254.3739 - val_mse: 254.2676\n",
      "Epoch 136/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.4414 - mse: 3.3350 - val_loss: 246.1921 - val_mse: 246.0851\n",
      "Epoch 137/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.3389 - mse: 3.2322 - val_loss: 269.0638 - val_mse: 268.9563\n",
      "Epoch 138/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.9424 - mse: 3.8353 - val_loss: 240.1078 - val_mse: 240.0004\n",
      "Epoch 139/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.4671 - mse: 3.3599 - val_loss: 245.8530 - val_mse: 245.7461\n",
      "Epoch 140/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.6099 - mse: 3.5024 - val_loss: 240.9621 - val_mse: 240.8544\n",
      "Epoch 141/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.3835 - mse: 3.2755 - val_loss: 234.5352 - val_mse: 234.4265\n",
      "Epoch 142/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.9182 - mse: 3.8096 - val_loss: 238.1837 - val_mse: 238.0752\n",
      "Epoch 143/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.6775 - mse: 3.5684 - val_loss: 237.1366 - val_mse: 237.0273\n",
      "Epoch 144/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.2099 - mse: 3.1004 - val_loss: 247.5807 - val_mse: 247.4711\n",
      "Epoch 145/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.7666 - mse: 3.6568 - val_loss: 266.4085 - val_mse: 266.2988\n",
      "Epoch 146/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.5030 - mse: 3.3930 - val_loss: 239.2433 - val_mse: 239.1328\n",
      "Epoch 147/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.4124 - mse: 3.3019 - val_loss: 236.3703 - val_mse: 236.2595\n",
      "Epoch 148/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.3811 - mse: 3.2702 - val_loss: 253.8139 - val_mse: 253.7022\n",
      "Epoch 149/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 4.0851 - mse: 3.9738 - val_loss: 243.7438 - val_mse: 243.6319\n",
      "Epoch 150/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.9042 - mse: 3.7923 - val_loss: 245.3097 - val_mse: 245.1972\n",
      "Epoch 151/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.8369 - mse: 3.7245 - val_loss: 228.5518 - val_mse: 228.4389\n",
      "Epoch 152/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.6897 - mse: 3.5770 - val_loss: 239.2636 - val_mse: 239.1508\n",
      "Epoch 153/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.2308 - mse: 3.1177 - val_loss: 233.7780 - val_mse: 233.6647\n",
      "Epoch 154/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.5525 - mse: 3.4390 - val_loss: 252.4256 - val_mse: 252.3123\n",
      "Epoch 155/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.3485 - mse: 3.2346 - val_loss: 235.7517 - val_mse: 235.6375\n",
      "Epoch 156/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.5956 - mse: 3.4813 - val_loss: 259.5669 - val_mse: 259.4523\n",
      "Epoch 157/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.5029 - mse: 3.3884 - val_loss: 240.6058 - val_mse: 240.4911\n",
      "Epoch 158/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.2963 - mse: 3.1813 - val_loss: 245.1526 - val_mse: 245.0372\n",
      "Epoch 159/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.2436 - mse: 3.1283 - val_loss: 236.9907 - val_mse: 236.8749\n",
      "Epoch 160/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.0376 - mse: 2.9220 - val_loss: 260.4643 - val_mse: 260.3483\n",
      "Epoch 161/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 3.8015 - mse: 3.6855 - val_loss: 246.9353 - val_mse: 246.8188\n",
      "Epoch 162/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 4.0087 - mse: 3.8923 - val_loss: 242.4412 - val_mse: 242.3246\n",
      "Epoch 163/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.6808 - mse: 3.5640 - val_loss: 238.1555 - val_mse: 238.0385\n",
      "Epoch 164/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.2920 - mse: 3.1749 - val_loss: 253.8395 - val_mse: 253.7221\n",
      "Epoch 165/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.2784 - mse: 3.1608 - val_loss: 234.6553 - val_mse: 234.5376\n",
      "Epoch 166/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.8402 - mse: 3.7223 - val_loss: 243.5786 - val_mse: 243.4607\n",
      "Epoch 167/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 4.0693 - mse: 3.9509 - val_loss: 252.2488 - val_mse: 252.1298\n",
      "Epoch 168/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 4.0488 - mse: 3.9299 - val_loss: 245.0868 - val_mse: 244.9677\n",
      "Epoch 169/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.7077 - mse: 3.5882 - val_loss: 251.0921 - val_mse: 250.9721\n",
      "Epoch 170/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.2265 - mse: 3.1067 - val_loss: 235.0998 - val_mse: 234.9799\n",
      "Epoch 171/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.5549 - mse: 3.4346 - val_loss: 256.9891 - val_mse: 256.8688\n",
      "Epoch 172/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.5435 - mse: 3.4228 - val_loss: 243.8999 - val_mse: 243.7789\n",
      "Epoch 173/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 3.2924 - mse: 3.1711 - val_loss: 245.4849 - val_mse: 245.3631\n",
      "Epoch 174/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.3123 - mse: 3.1907 - val_loss: 239.8736 - val_mse: 239.7519\n",
      "Epoch 175/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.1611 - mse: 3.0391 - val_loss: 249.9085 - val_mse: 249.7864\n",
      "Epoch 176/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.3607 - mse: 3.2384 - val_loss: 238.1265 - val_mse: 238.0040\n",
      "Epoch 177/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.2520 - mse: 3.1294 - val_loss: 265.7592 - val_mse: 265.6366\n",
      "Epoch 178/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 3.2458 - mse: 3.1230 - val_loss: 233.9561 - val_mse: 233.8331\n",
      "Epoch 179/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.4873 - mse: 3.3640 - val_loss: 247.0211 - val_mse: 246.8978\n",
      "Epoch 180/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.6407 - mse: 3.5171 - val_loss: 233.3735 - val_mse: 233.2494\n",
      "Epoch 181/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.2919 - mse: 3.1678 - val_loss: 263.2468 - val_mse: 263.1225\n",
      "Epoch 182/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 3.3189 - mse: 3.1946 - val_loss: 227.5995 - val_mse: 227.4752\n",
      "Epoch 183/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.3144 - mse: 3.1899 - val_loss: 241.4939 - val_mse: 241.3693\n",
      "Epoch 184/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.4897 - mse: 3.3651 - val_loss: 243.3507 - val_mse: 243.2259\n",
      "Epoch 185/500\n",
      "8288/8288 [==============================] - 134s 16ms/step - loss: 3.2763 - mse: 3.1514 - val_loss: 241.7136 - val_mse: 241.5887\n",
      "Epoch 186/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.2700 - mse: 3.1451 - val_loss: 237.8801 - val_mse: 237.7551\n",
      "Epoch 187/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.2435 - mse: 3.1184 - val_loss: 252.0296 - val_mse: 251.9044\n",
      "Epoch 188/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.5078 - mse: 3.3825 - val_loss: 250.1437 - val_mse: 250.0182\n",
      "Epoch 189/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.3399 - mse: 3.2143 - val_loss: 251.5006 - val_mse: 251.3748\n",
      "Epoch 190/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.8874 - mse: 2.7615 - val_loss: 231.3671 - val_mse: 231.2411\n",
      "Epoch 191/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.1597 - mse: 3.0337 - val_loss: 253.9344 - val_mse: 253.8086\n",
      "Epoch 192/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.0395 - mse: 2.9133 - val_loss: 232.7249 - val_mse: 232.5984\n",
      "Epoch 193/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.0964 - mse: 2.9698 - val_loss: 239.2852 - val_mse: 239.1583\n",
      "Epoch 194/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.4945 - mse: 3.3678 - val_loss: 250.1482 - val_mse: 250.0214\n",
      "Epoch 195/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.8705 - mse: 2.7436 - val_loss: 246.3621 - val_mse: 246.2351\n",
      "Epoch 196/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.7765 - mse: 3.6494 - val_loss: 241.8405 - val_mse: 241.7132\n",
      "Epoch 197/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.3454 - mse: 3.2179 - val_loss: 255.1019 - val_mse: 254.9741\n",
      "Epoch 198/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.2242 - mse: 3.0964 - val_loss: 246.7019 - val_mse: 246.5741\n",
      "Epoch 199/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.4996 - mse: 3.3714 - val_loss: 246.8351 - val_mse: 246.7065\n",
      "Epoch 200/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.1839 - mse: 3.0556 - val_loss: 250.4525 - val_mse: 250.3239\n",
      "Epoch 201/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.0948 - mse: 2.9663 - val_loss: 230.5131 - val_mse: 230.3844\n",
      "Epoch 202/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.0392 - mse: 2.9105 - val_loss: 239.2347 - val_mse: 239.1061\n",
      "Epoch 203/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.1004 - mse: 2.9716 - val_loss: 234.8769 - val_mse: 234.7479\n",
      "Epoch 204/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.4764 - mse: 3.3474 - val_loss: 242.0776 - val_mse: 241.9485\n",
      "Epoch 205/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.7299 - mse: 3.6005 - val_loss: 235.7407 - val_mse: 235.6111\n",
      "Epoch 206/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.3491 - mse: 3.2195 - val_loss: 238.8909 - val_mse: 238.7613\n",
      "Epoch 207/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.3514 - mse: 3.2217 - val_loss: 238.7219 - val_mse: 238.5920\n",
      "Epoch 208/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.5849 - mse: 3.4548 - val_loss: 239.2728 - val_mse: 239.1429\n",
      "Epoch 209/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.1304 - mse: 3.0004 - val_loss: 252.6872 - val_mse: 252.5571\n",
      "Epoch 210/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.1591 - mse: 3.0289 - val_loss: 244.5480 - val_mse: 244.4175\n",
      "Epoch 211/500\n",
      "8288/8288 [==============================] - 134s 16ms/step - loss: 3.4221 - mse: 3.2916 - val_loss: 247.2370 - val_mse: 247.1063\n",
      "Epoch 212/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.1317 - mse: 3.0008 - val_loss: 248.6807 - val_mse: 248.5495\n",
      "Epoch 213/500\n",
      "8288/8288 [==============================] - 147s 18ms/step - loss: 3.4826 - mse: 3.3513 - val_loss: 236.8497 - val_mse: 236.7189\n",
      "Epoch 214/500\n",
      "8288/8288 [==============================] - 146s 18ms/step - loss: 3.1033 - mse: 2.9717 - val_loss: 239.4711 - val_mse: 239.3393\n",
      "Epoch 215/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.1830 - mse: 3.0511 - val_loss: 237.7531 - val_mse: 237.6210\n",
      "Epoch 216/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 3.1003 - mse: 2.9683 - val_loss: 241.1338 - val_mse: 241.0013\n",
      "Epoch 217/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 3.0386 - mse: 2.9064 - val_loss: 238.2209 - val_mse: 238.0880\n",
      "Epoch 218/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.9037 - mse: 2.7712 - val_loss: 240.5519 - val_mse: 240.4193\n",
      "Epoch 219/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 3.3512 - mse: 3.2185 - val_loss: 251.7390 - val_mse: 251.6066\n",
      "Epoch 220/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 3.1960 - mse: 3.0631 - val_loss: 252.7021 - val_mse: 252.5689\n",
      "Epoch 221/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.2745 - mse: 3.1414 - val_loss: 247.3934 - val_mse: 247.2599\n",
      "Epoch 222/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.0071 - mse: 2.8736 - val_loss: 242.5873 - val_mse: 242.4538\n",
      "Epoch 223/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 2.9497 - mse: 2.8160 - val_loss: 240.7733 - val_mse: 240.6395\n",
      "Epoch 224/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 2.8279 - mse: 2.6941 - val_loss: 229.5858 - val_mse: 229.4519\n",
      "Epoch 225/500\n",
      "8288/8288 [==============================] - 145s 17ms/step - loss: 3.0430 - mse: 2.9090 - val_loss: 245.6999 - val_mse: 245.5658\n",
      "Epoch 226/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 3.1857 - mse: 3.0515 - val_loss: 241.9635 - val_mse: 241.8290\n",
      "Epoch 227/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 2.9493 - mse: 2.8148 - val_loss: 254.8871 - val_mse: 254.7523\n",
      "Epoch 228/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 2.8322 - mse: 2.6976 - val_loss: 234.1653 - val_mse: 234.0306\n",
      "Epoch 229/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 3.0553 - mse: 2.9206 - val_loss: 235.9740 - val_mse: 235.8394\n",
      "Epoch 230/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 3.8620 - mse: 3.7271 - val_loss: 251.2543 - val_mse: 251.1195\n",
      "Epoch 231/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 3.3944 - mse: 3.2590 - val_loss: 250.2133 - val_mse: 250.0777\n",
      "Epoch 232/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 2.9881 - mse: 2.8524 - val_loss: 234.9275 - val_mse: 234.7912\n",
      "Epoch 233/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 2.8868 - mse: 2.7508 - val_loss: 236.5112 - val_mse: 236.3755\n",
      "Epoch 234/500\n",
      "8288/8288 [==============================] - 162s 20ms/step - loss: 3.0756 - mse: 2.9394 - val_loss: 265.8063 - val_mse: 265.6700\n",
      "Epoch 235/500\n",
      "8288/8288 [==============================] - 182s 22ms/step - loss: 3.2892 - mse: 3.1529 - val_loss: 238.3851 - val_mse: 238.2485\n",
      "Epoch 236/500\n",
      "8288/8288 [==============================] - 159s 19ms/step - loss: 2.9183 - mse: 2.7816 - val_loss: 241.0348 - val_mse: 240.8983\n",
      "Epoch 237/500\n",
      "8288/8288 [==============================] - 178s 21ms/step - loss: 3.4204 - mse: 3.2834 - val_loss: 247.2092 - val_mse: 247.0727\n",
      "Epoch 238/500\n",
      "8288/8288 [==============================] - 166s 20ms/step - loss: 3.3617 - mse: 3.2245 - val_loss: 239.6062 - val_mse: 239.4691\n",
      "Epoch 239/500\n",
      "8288/8288 [==============================] - 172s 21ms/step - loss: 3.3988 - mse: 3.2613 - val_loss: 234.4925 - val_mse: 234.3548\n",
      "Epoch 240/500\n",
      "8288/8288 [==============================] - 172s 21ms/step - loss: 3.3753 - mse: 3.2375 - val_loss: 247.2631 - val_mse: 247.1246\n",
      "Epoch 241/500\n",
      "8288/8288 [==============================] - 168s 20ms/step - loss: 3.2419 - mse: 3.1037 - val_loss: 253.8879 - val_mse: 253.7498\n",
      "Epoch 242/500\n",
      "8288/8288 [==============================] - 162s 20ms/step - loss: 3.5561 - mse: 3.4177 - val_loss: 251.0280 - val_mse: 250.8893\n",
      "Epoch 243/500\n",
      "8288/8288 [==============================] - 173s 21ms/step - loss: 3.0397 - mse: 2.9009 - val_loss: 242.9650 - val_mse: 242.8262\n",
      "Epoch 244/500\n",
      "8288/8288 [==============================] - 162s 20ms/step - loss: 3.5315 - mse: 3.3925 - val_loss: 241.0639 - val_mse: 240.9247\n",
      "Epoch 245/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.9930 - mse: 2.8540 - val_loss: 238.9624 - val_mse: 238.8236\n",
      "Epoch 246/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 3.0161 - mse: 2.8770 - val_loss: 238.9004 - val_mse: 238.7608\n",
      "Epoch 247/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 3.3017 - mse: 3.1623 - val_loss: 228.6582 - val_mse: 228.5187\n",
      "Epoch 248/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.2833 - mse: 3.1436 - val_loss: 239.7233 - val_mse: 239.5838\n",
      "Epoch 249/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.2330 - mse: 3.0931 - val_loss: 244.8199 - val_mse: 244.6799\n",
      "Epoch 250/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 3.2228 - mse: 3.0829 - val_loss: 242.2401 - val_mse: 242.1000\n",
      "Epoch 251/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.8796 - mse: 2.7394 - val_loss: 249.8786 - val_mse: 249.7383\n",
      "Epoch 252/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.3654 - mse: 3.2250 - val_loss: 241.3302 - val_mse: 241.1895\n",
      "Epoch 253/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.0131 - mse: 2.8726 - val_loss: 235.0841 - val_mse: 234.9437\n",
      "Epoch 254/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 2.7352 - mse: 2.5945 - val_loss: 239.5680 - val_mse: 239.4265\n",
      "Epoch 255/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.5090 - mse: 3.3681 - val_loss: 230.1037 - val_mse: 229.9630\n",
      "Epoch 256/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.8071 - mse: 2.6662 - val_loss: 234.9485 - val_mse: 234.8073\n",
      "Epoch 257/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.3442 - mse: 3.2031 - val_loss: 240.6242 - val_mse: 240.4831\n",
      "Epoch 258/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 3.1808 - mse: 3.0395 - val_loss: 233.1907 - val_mse: 233.0494\n",
      "Epoch 259/500\n",
      "8288/8288 [==============================] - 138s 17ms/step - loss: 3.2809 - mse: 3.1394 - val_loss: 236.1740 - val_mse: 236.0329\n",
      "Epoch 260/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.8131 - mse: 2.6714 - val_loss: 227.8495 - val_mse: 227.7074\n",
      "Epoch 261/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 2.8662 - mse: 2.7243 - val_loss: 259.2862 - val_mse: 259.1443\n",
      "Epoch 262/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 2.9289 - mse: 2.7866 - val_loss: 251.3430 - val_mse: 251.2008\n",
      "Epoch 263/500\n",
      "8288/8288 [==============================] - 154s 19ms/step - loss: 2.9456 - mse: 2.8031 - val_loss: 234.5505 - val_mse: 234.4082\n",
      "Epoch 264/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.9214 - mse: 2.7789 - val_loss: 232.6185 - val_mse: 232.4759\n",
      "Epoch 265/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 2.7595 - mse: 2.6169 - val_loss: 236.9963 - val_mse: 236.8537\n",
      "Epoch 266/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.7026 - mse: 2.5598 - val_loss: 223.1172 - val_mse: 222.9743\n",
      "Epoch 267/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 2.7603 - mse: 2.6175 - val_loss: 238.3815 - val_mse: 238.2384\n",
      "Epoch 268/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.6341 - mse: 2.4912 - val_loss: 242.7028 - val_mse: 242.5602\n",
      "Epoch 269/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 2.7644 - mse: 2.6218 - val_loss: 253.8977 - val_mse: 253.7550\n",
      "Epoch 270/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.8294 - mse: 2.6868 - val_loss: 224.7072 - val_mse: 224.5641\n",
      "Epoch 271/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.9723 - mse: 2.8297 - val_loss: 238.3732 - val_mse: 238.2312\n",
      "Epoch 272/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 2.9710 - mse: 2.8282 - val_loss: 254.0784 - val_mse: 253.9354\n",
      "Epoch 273/500\n",
      "8288/8288 [==============================] - 133s 16ms/step - loss: 2.7360 - mse: 2.5931 - val_loss: 245.9267 - val_mse: 245.7833\n",
      "Epoch 274/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.7537 - mse: 2.6107 - val_loss: 237.4437 - val_mse: 237.3003\n",
      "Epoch 275/500\n",
      "8288/8288 [==============================] - 137s 16ms/step - loss: 2.7965 - mse: 2.6534 - val_loss: 240.2453 - val_mse: 240.1023\n",
      "Epoch 276/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.3057 - mse: 3.1623 - val_loss: 239.4815 - val_mse: 239.3383\n",
      "Epoch 277/500\n",
      "8288/8288 [==============================] - 149s 18ms/step - loss: 2.7868 - mse: 2.6433 - val_loss: 239.7533 - val_mse: 239.6094\n",
      "Epoch 278/500\n",
      "8288/8288 [==============================] - 149s 18ms/step - loss: 3.2830 - mse: 3.1394 - val_loss: 234.6470 - val_mse: 234.5030\n",
      "Epoch 279/500\n",
      "8288/8288 [==============================] - 147s 18ms/step - loss: 2.9394 - mse: 2.7956 - val_loss: 231.0649 - val_mse: 230.9211\n",
      "Epoch 280/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 2.7093 - mse: 2.5653 - val_loss: 230.8508 - val_mse: 230.7063\n",
      "Epoch 281/500\n",
      "8288/8288 [==============================] - 148s 18ms/step - loss: 2.7528 - mse: 2.6084 - val_loss: 233.5500 - val_mse: 233.4056\n",
      "Epoch 282/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.7701 - mse: 2.6256 - val_loss: 239.5888 - val_mse: 239.4439\n",
      "Epoch 283/500\n",
      "8288/8288 [==============================] - 145s 17ms/step - loss: 2.7938 - mse: 2.6493 - val_loss: 236.7804 - val_mse: 236.6360\n",
      "Epoch 284/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 3.6185 - mse: 3.4739 - val_loss: 247.7827 - val_mse: 247.6380\n",
      "Epoch 285/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 3.6408 - mse: 3.4961 - val_loss: 235.5936 - val_mse: 235.4491\n",
      "Epoch 286/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.2437 - mse: 3.0989 - val_loss: 251.7622 - val_mse: 251.6174\n",
      "Epoch 287/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 3.3279 - mse: 3.1831 - val_loss: 232.9733 - val_mse: 232.8285\n",
      "Epoch 288/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.2873 - mse: 3.1423 - val_loss: 250.6793 - val_mse: 250.5344\n",
      "Epoch 289/500\n",
      "8288/8288 [==============================] - 163s 19ms/step - loss: 2.9982 - mse: 2.8530 - val_loss: 226.8465 - val_mse: 226.7013\n",
      "Epoch 290/500\n",
      "8288/8288 [==============================] - 139s 17ms/step - loss: 2.6829 - mse: 2.5376 - val_loss: 239.3268 - val_mse: 239.1812\n",
      "Epoch 291/500\n",
      "8288/8288 [==============================] - 145s 17ms/step - loss: 3.1263 - mse: 2.9807 - val_loss: 235.8933 - val_mse: 235.7475\n",
      "Epoch 292/500\n",
      "8288/8288 [==============================] - 146s 18ms/step - loss: 3.1832 - mse: 3.0374 - val_loss: 236.0434 - val_mse: 235.8974\n",
      "Epoch 293/500\n",
      "8288/8288 [==============================] - 157s 19ms/step - loss: 2.7783 - mse: 2.6324 - val_loss: 226.1143 - val_mse: 225.9679\n",
      "Epoch 294/500\n",
      "8288/8288 [==============================] - 158s 19ms/step - loss: 3.0467 - mse: 2.9005 - val_loss: 245.5763 - val_mse: 245.4302\n",
      "Epoch 295/500\n",
      "8288/8288 [==============================] - 177s 21ms/step - loss: 2.9021 - mse: 2.7559 - val_loss: 239.3386 - val_mse: 239.1921\n",
      "Epoch 296/500\n",
      "8288/8288 [==============================] - 230s 28ms/step - loss: 2.7914 - mse: 2.6451 - val_loss: 243.1717 - val_mse: 243.0251\n",
      "Epoch 297/500\n",
      "8288/8288 [==============================] - 141s 17ms/step - loss: 2.8252 - mse: 2.6787 - val_loss: 227.5362 - val_mse: 227.3895\n",
      "Epoch 298/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 3.0586 - mse: 2.9119 - val_loss: 248.5339 - val_mse: 248.3874\n",
      "Epoch 299/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 2.6201 - mse: 2.4733 - val_loss: 248.2323 - val_mse: 248.0851\n",
      "Epoch 300/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 3.3801 - mse: 3.2331 - val_loss: 239.7433 - val_mse: 239.5965\n",
      "Epoch 301/500\n",
      "8288/8288 [==============================] - 131s 16ms/step - loss: 2.7653 - mse: 2.6182 - val_loss: 234.2323 - val_mse: 234.0855\n",
      "Epoch 302/500\n",
      "8288/8288 [==============================] - 128s 15ms/step - loss: 2.9853 - mse: 2.8381 - val_loss: 235.5938 - val_mse: 235.4469\n",
      "Epoch 303/500\n",
      "8288/8288 [==============================] - 124s 15ms/step - loss: 2.8190 - mse: 2.6715 - val_loss: 240.0764 - val_mse: 239.9290\n",
      "Epoch 304/500\n",
      "8288/8288 [==============================] - 128s 15ms/step - loss: 2.6925 - mse: 2.5448 - val_loss: 245.7863 - val_mse: 245.6385\n",
      "Epoch 305/500\n",
      "8288/8288 [==============================] - 129s 16ms/step - loss: 2.9471 - mse: 2.7993 - val_loss: 243.0992 - val_mse: 242.9512\n",
      "Epoch 306/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 2.7131 - mse: 2.5651 - val_loss: 233.8949 - val_mse: 233.7471\n",
      "Epoch 307/500\n",
      "8288/8288 [==============================] - 132s 16ms/step - loss: 2.8395 - mse: 2.6913 - val_loss: 239.7016 - val_mse: 239.5535\n",
      "Epoch 308/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 2.6893 - mse: 2.5409 - val_loss: 235.3380 - val_mse: 235.1894\n",
      "Epoch 309/500\n",
      "8288/8288 [==============================] - 136s 16ms/step - loss: 2.6531 - mse: 2.5046 - val_loss: 231.7171 - val_mse: 231.5682\n",
      "Epoch 310/500\n",
      "8288/8288 [==============================] - 137s 17ms/step - loss: 3.0920 - mse: 2.9435 - val_loss: 234.0484 - val_mse: 233.8995\n",
      "Epoch 311/500\n",
      "8288/8288 [==============================] - 124s 15ms/step - loss: 2.5721 - mse: 2.4235 - val_loss: 236.4362 - val_mse: 236.2874\n",
      "Epoch 312/500\n",
      "8288/8288 [==============================] - 130s 16ms/step - loss: 2.8639 - mse: 2.7152 - val_loss: 241.1519 - val_mse: 241.0035\n",
      "Epoch 313/500\n",
      "8288/8288 [==============================] - 129s 16ms/step - loss: 2.7638 - mse: 2.6150 - val_loss: 237.5444 - val_mse: 237.3955\n",
      "Epoch 314/500\n",
      "8288/8288 [==============================] - 129s 16ms/step - loss: 2.7280 - mse: 2.5791 - val_loss: 247.2840 - val_mse: 247.1350\n",
      "Epoch 315/500\n",
      "8288/8288 [==============================] - 124s 15ms/step - loss: 2.8774 - mse: 2.7284 - val_loss: 238.9309 - val_mse: 238.7819\n",
      "Epoch 316/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.8910 - mse: 2.7419 - val_loss: 236.5962 - val_mse: 236.4468\n",
      "Epoch 317/500\n",
      "8288/8288 [==============================] - 178s 22ms/step - loss: 2.7581 - mse: 2.6090 - val_loss: 240.4282 - val_mse: 240.2792\n",
      "Epoch 318/500\n",
      "8288/8288 [==============================] - 168s 20ms/step - loss: 2.7037 - mse: 2.5546 - val_loss: 233.8399 - val_mse: 233.6906\n",
      "Epoch 319/500\n",
      "8288/8288 [==============================] - 163s 20ms/step - loss: 2.9343 - mse: 2.7849 - val_loss: 239.3646 - val_mse: 239.2151\n",
      "Epoch 320/500\n",
      "8288/8288 [==============================] - 155s 19ms/step - loss: 3.1820 - mse: 3.0326 - val_loss: 226.5614 - val_mse: 226.4119\n",
      "Epoch 321/500\n",
      "8288/8288 [==============================] - 161s 19ms/step - loss: 3.9889 - mse: 3.8394 - val_loss: 237.7049 - val_mse: 237.5552\n",
      "Epoch 322/500\n",
      "8288/8288 [==============================] - 156s 19ms/step - loss: 4.0897 - mse: 3.9401 - val_loss: 231.1153 - val_mse: 230.9659\n",
      "Epoch 323/500\n",
      "8288/8288 [==============================] - 155s 19ms/step - loss: 3.0748 - mse: 2.9251 - val_loss: 243.4443 - val_mse: 243.2947\n",
      "Epoch 324/500\n",
      "8288/8288 [==============================] - 165s 20ms/step - loss: 2.8772 - mse: 2.7273 - val_loss: 238.6374 - val_mse: 238.4875\n",
      "Epoch 325/500\n",
      "8288/8288 [==============================] - 165s 20ms/step - loss: 2.7796 - mse: 2.6297 - val_loss: 230.2222 - val_mse: 230.0721\n",
      "Epoch 326/500\n",
      "8288/8288 [==============================] - 170s 21ms/step - loss: 2.6811 - mse: 2.5312 - val_loss: 248.9990 - val_mse: 248.8493\n",
      "Epoch 327/500\n",
      "8288/8288 [==============================] - 182s 22ms/step - loss: 2.8609 - mse: 2.7110 - val_loss: 236.4964 - val_mse: 236.3464\n",
      "Epoch 328/500\n",
      "8288/8288 [==============================] - 152s 18ms/step - loss: 2.6226 - mse: 2.4725 - val_loss: 232.2555 - val_mse: 232.1054\n",
      "Epoch 329/500\n",
      "8288/8288 [==============================] - 152s 18ms/step - loss: 3.0039 - mse: 2.8538 - val_loss: 228.6138 - val_mse: 228.4639\n",
      "Epoch 330/500\n",
      "8288/8288 [==============================] - 197s 24ms/step - loss: 3.0761 - mse: 2.9261 - val_loss: 239.9595 - val_mse: 239.8093\n",
      "Epoch 331/500\n",
      "8288/8288 [==============================] - 209s 25ms/step - loss: 3.0355 - mse: 2.8854 - val_loss: 239.6219 - val_mse: 239.4713\n",
      "Epoch 332/500\n",
      "8288/8288 [==============================] - 194s 23ms/step - loss: 3.0479 - mse: 2.8975 - val_loss: 243.1481 - val_mse: 242.9975\n",
      "Epoch 333/500\n",
      "8288/8288 [==============================] - 194s 23ms/step - loss: 2.9764 - mse: 2.8260 - val_loss: 234.2346 - val_mse: 234.0844\n",
      "Epoch 334/500\n",
      "8288/8288 [==============================] - 189s 23ms/step - loss: 2.9673 - mse: 2.8167 - val_loss: 250.4373 - val_mse: 250.2870\n",
      "Epoch 335/500\n",
      "8288/8288 [==============================] - 196s 24ms/step - loss: 2.8066 - mse: 2.6558 - val_loss: 235.0472 - val_mse: 234.8965\n",
      "Epoch 336/500\n",
      "8288/8288 [==============================] - 178s 22ms/step - loss: 2.7668 - mse: 2.6159 - val_loss: 271.5294 - val_mse: 271.3782\n",
      "Epoch 337/500\n",
      "8288/8288 [==============================] - 175s 21ms/step - loss: 2.7306 - mse: 2.5796 - val_loss: 256.5328 - val_mse: 256.3817\n",
      "Epoch 338/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.7025 - mse: 2.5514 - val_loss: 244.6501 - val_mse: 244.4989\n",
      "Epoch 339/500\n",
      "8288/8288 [==============================] - 143s 17ms/step - loss: 2.6925 - mse: 2.5413 - val_loss: 229.0341 - val_mse: 228.8828\n",
      "Epoch 340/500\n",
      "8288/8288 [==============================] - 159s 19ms/step - loss: 2.6931 - mse: 2.5418 - val_loss: 241.9037 - val_mse: 241.7523\n",
      "Epoch 341/500\n",
      "8288/8288 [==============================] - 164s 20ms/step - loss: 2.5238 - mse: 2.3725 - val_loss: 239.6089 - val_mse: 239.4573\n",
      "Epoch 342/500\n",
      "8288/8288 [==============================] - 144s 17ms/step - loss: 2.7877 - mse: 2.6362 - val_loss: 247.9655 - val_mse: 247.8140\n",
      "Epoch 343/500\n",
      "8288/8288 [==============================] - 147s 18ms/step - loss: 2.7261 - mse: 2.5746 - val_loss: 233.2557 - val_mse: 233.1041\n",
      "Epoch 344/500\n",
      "8288/8288 [==============================] - 145s 17ms/step - loss: 3.1710 - mse: 3.0193 - val_loss: 241.1530 - val_mse: 241.0013\n",
      "Epoch 345/500\n",
      "8288/8288 [==============================] - 142s 17ms/step - loss: 3.8954 - mse: 3.7437 - val_loss: 235.8921 - val_mse: 235.7406\n",
      "Epoch 346/500\n",
      "8288/8288 [==============================] - 135s 16ms/step - loss: 2.6415 - mse: 2.4897 - val_loss: 243.2857 - val_mse: 243.1336\n",
      "Epoch 347/500\n",
      "8288/8288 [==============================] - 195s 24ms/step - loss: 2.8346 - mse: 2.6827 - val_loss: 234.0775 - val_mse: 233.9252\n",
      "Epoch 348/500\n",
      "8288/8288 [==============================] - 179s 22ms/step - loss: 2.7880 - mse: 2.6360 - val_loss: 275.9601 - val_mse: 275.8080\n",
      "Epoch 349/500\n",
      "8288/8288 [==============================] - 153s 18ms/step - loss: 3.0179 - mse: 2.8660 - val_loss: 238.2757 - val_mse: 238.1238\n",
      "Epoch 350/500\n",
      "8288/8288 [==============================] - 146s 18ms/step - loss: 2.6754 - mse: 2.5235 - val_loss: 246.8442 - val_mse: 246.6922\n",
      "Epoch 351/500\n",
      "8288/8288 [==============================] - 150s 18ms/step - loss: 2.7591 - mse: 2.6071 - val_loss: 240.6421 - val_mse: 240.4902\n",
      "Epoch 352/500\n",
      "8288/8288 [==============================] - 149s 18ms/step - loss: 2.7710 - mse: 2.6190 - val_loss: 237.9669 - val_mse: 237.8149\n",
      "Epoch 353/500\n",
      "8288/8288 [==============================] - 160s 19ms/step - loss: 2.6522 - mse: 2.5001 - val_loss: 249.9646 - val_mse: 249.8122\n",
      "Epoch 354/500\n",
      "8288/8288 [==============================] - 187s 23ms/step - loss: 2.5715 - mse: 2.4194 - val_loss: 240.5497 - val_mse: 240.3976\n",
      "Epoch 355/500\n",
      "8288/8288 [==============================] - 152s 18ms/step - loss: 2.6099 - mse: 2.4578 - val_loss: 237.1835 - val_mse: 237.0316\n",
      "Epoch 356/500\n",
      "8288/8288 [==============================] - 170s 21ms/step - loss: 2.8039 - mse: 2.6519 - val_loss: 238.9374 - val_mse: 238.7850\n",
      "Epoch 357/500\n",
      "8288/8288 [==============================] - 157s 19ms/step - loss: 2.6692 - mse: 2.5169 - val_loss: 239.7451 - val_mse: 239.5930\n",
      "Epoch 358/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 2.5783 - mse: 2.4259 - val_loss: 245.4421 - val_mse: 245.2894\n",
      "Epoch 359/500\n",
      "8288/8288 [==============================] - 140s 17ms/step - loss: 2.8572 - mse: 2.7048 - val_loss: 240.5204 - val_mse: 240.3678\n",
      "Epoch 360/500\n",
      "8288/8288 [==============================] - 174s 21ms/step - loss: 2.6063 - mse: 2.4539 - val_loss: 261.2790 - val_mse: 261.1265\n",
      "Epoch 361/500\n",
      "8288/8288 [==============================] - 166s 20ms/step - loss: 2.7243 - mse: 2.5718 - val_loss: 233.1212 - val_mse: 232.9687\n",
      "Epoch 362/500\n",
      "8288/8288 [==============================] - 181s 22ms/step - loss: 2.6362 - mse: 2.4838 - val_loss: 238.0144 - val_mse: 237.8621\n",
      "Epoch 363/500\n",
      "8288/8288 [==============================] - 166s 20ms/step - loss: 2.6582 - mse: 2.5056 - val_loss: 240.3957 - val_mse: 240.2429\n",
      "Epoch 364/500\n",
      "8288/8288 [==============================] - 175s 21ms/step - loss: 2.5182 - mse: 2.3655 - val_loss: 241.1681 - val_mse: 241.0156\n",
      "Epoch 365/500\n",
      "8288/8288 [==============================] - 161s 19ms/step - loss: 2.9029 - mse: 2.7501 - val_loss: 245.3642 - val_mse: 245.2114\n",
      "Epoch 366/500\n",
      " 668/8288 [=>............................] - ETA: 1:48 - loss: 3.0529 - mse: 2.9000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-12e3965e1bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter_valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\mathe\\anaconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=iter_train,\n",
    "        validation_steps=iter_valid\n",
    "    )\n",
    "\n",
    "model.save('result.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
