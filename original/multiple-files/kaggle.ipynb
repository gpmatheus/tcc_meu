{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c2c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:15.474406Z",
     "iopub.status.busy": "2025-05-20T01:17:15.474180Z",
     "iopub.status.idle": "2025-05-20T01:17:29.313882Z",
     "shell.execute_reply": "2025-05-20T01:17:29.313306Z"
    },
    "papermill": {
     "duration": 13.845632,
     "end_time": "2025-05-20T01:17:29.315348",
     "exception": false,
     "start_time": "2025-05-20T01:17:15.469716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96e946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.323425Z",
     "iopub.status.busy": "2025-05-20T01:17:29.322787Z",
     "iopub.status.idle": "2025-05-20T01:17:29.328180Z",
     "shell.execute_reply": "2025-05-20T01:17:29.327572Z"
    },
    "papermill": {
     "duration": 0.010195,
     "end_time": "2025-05-20T01:17:29.329214",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.319019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed = 1747265027\n",
    "seed = int(time.time()) % (2**32 - 1)  # ou: random.randint(0, 999999)\n",
    "print(f\"Usando seed: {seed}\")\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa8805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.336471Z",
     "iopub.status.busy": "2025-05-20T01:17:29.335949Z",
     "iopub.status.idle": "2025-05-20T01:17:29.339923Z",
     "shell.execute_reply": "2025-05-20T01:17:29.339205Z"
    },
    "papermill": {
     "duration": 0.00858,
     "end_time": "2025-05-20T01:17:29.340952",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.332372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path, valid_path, test_path = '/kaggle/working/train.h5', '/kaggle/working/valid.h5', '/kaggle/working/test.h5'\n",
    "ds = [f'/kaggle/input/tcir-atln-epac-wpac-h5/{i}' for i in ['TCIR-ATLN_EPAC_WPAC.h5', 'TCIR-CPAC_IO_SH.h5']]\n",
    "channels = [0, 3]\n",
    "img_w = 64\n",
    "load_batch = 4096\n",
    "epochs = 500\n",
    "batch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d8971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.347788Z",
     "iopub.status.busy": "2025-05-20T01:17:29.347564Z",
     "iopub.status.idle": "2025-05-20T01:17:29.351071Z",
     "shell.execute_reply": "2025-05-20T01:17:29.350355Z"
    },
    "papermill": {
     "duration": 0.008068,
     "end_time": "2025-05-20T01:17:29.352070",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.344002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_images_slice(images_shape, width):\n",
    "    start = images_shape[1] // 2 - width // 2\n",
    "    end = images_shape[1] // 2 + width // 2\n",
    "    return slice(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f3e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.358870Z",
     "iopub.status.busy": "2025-05-20T01:17:29.358662Z",
     "iopub.status.idle": "2025-05-20T01:17:29.361944Z",
     "shell.execute_reply": "2025-05-20T01:17:29.361390Z"
    },
    "papermill": {
     "duration": 0.007733,
     "end_time": "2025-05-20T01:17:29.362906",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.355173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_images(images, width):\n",
    "    slc = get_images_slice(images.shape, width)\n",
    "    return images[:, slc, slc, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda43dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.369714Z",
     "iopub.status.busy": "2025-05-20T01:17:29.369173Z",
     "iopub.status.idle": "2025-05-20T01:17:29.372631Z",
     "shell.execute_reply": "2025-05-20T01:17:29.371947Z"
    },
    "papermill": {
     "duration": 0.007902,
     "end_time": "2025-05-20T01:17:29.373749",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.365847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_images(images):\n",
    "    images = np.nan_to_num(images, copy=False)\n",
    "    images[images > 1000] = 0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1906fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T01:17:29.392074Z",
     "iopub.status.busy": "2025-05-20T01:17:29.391869Z",
     "iopub.status.idle": "2025-05-20T01:17:29.396068Z",
     "shell.execute_reply": "2025-05-20T01:17:29.395580Z"
    },
    "papermill": {
     "duration": 0.00869,
     "end_time": "2025-05-20T01:17:29.397115",
     "exception": false,
     "start_time": "2025-05-20T01:17:29.388425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(images, info):\n",
    "    years = [datetime.datetime.strptime(i, \"%Y%m%d%H\").year for i in list(info['time'])]\n",
    "    years = np.array(years)\n",
    "    train_values = (years >= 2003) & (years <= 2014)\n",
    "    valid_values = (years >= 2015) & (years <= 2016)\n",
    "    train_idx = np.where(train_values)[0]\n",
    "    valid_idx = np.where(valid_values)[0]\n",
    "    info = info['Vmax'].to_numpy()\n",
    "    train_img, train_info = images[train_idx], info[train_idx]\n",
    "    valid_img, valid_info = images[valid_idx], info[valid_idx]\n",
    "    return (train_img, train_info), (valid_img, valid_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7448cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:14:03.233233Z",
     "iopub.status.busy": "2025-05-19T18:14:03.233027Z",
     "iopub.status.idle": "2025-05-19T18:22:54.697983Z",
     "shell.execute_reply": "2025-05-19T18:22:54.697229Z",
     "shell.execute_reply.started": "2025-05-19T18:14:03.233218Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-05-20T01:17:29.400238",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean(files, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum(train_image[:, :, :, j])\n",
    "    means = accumulators / (files_data_len * width * width)\n",
    "    return means\n",
    "\n",
    "mean = get_mean(ds, batch=load_batch, width=img_w)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9d3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:22:54.698835Z",
     "iopub.status.busy": "2025-05-19T18:22:54.698659Z",
     "iopub.status.idle": "2025-05-19T18:31:27.305780Z",
     "shell.execute_reply": "2025-05-19T18:31:27.304785Z",
     "shell.execute_reply.started": "2025-05-19T18:22:54.698822Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_std(files, mean, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum((train_image[:, :, :, j] - mean[j]) ** 2)\n",
    "    stds = accumulators / (files_data_len * width * width)\n",
    "    stds = np.sqrt(stds)\n",
    "    return stds\n",
    "\n",
    "std = get_std(ds, mean, batch=load_batch, width=img_w)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53402c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:31:27.307053Z",
     "iopub.status.busy": "2025-05-19T18:31:27.306712Z",
     "iopub.status.idle": "2025-05-19T18:40:14.516376Z",
     "shell.execute_reply": "2025-05-19T18:40:14.515726Z",
     "shell.execute_reply.started": "2025-05-19T18:31:27.307027Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def pre_process(files, width, means, stds, batch=1024):\n",
    "    # corta imagem grande o suficiente para poder rotacionar\n",
    "    rotation_width = int(np.ceil(np.sqrt((width ** 2) * 2)))\n",
    "    if rotation_width % 2 != 0:\n",
    "        rotation_width += 1\n",
    "\n",
    "    files_data_len = np.zeros(len(files))\n",
    "    with h5py.File(train_path, 'w') as train, h5py.File(valid_path, 'w') as valid:\n",
    "        for fi, file in enumerate(files):\n",
    "            with h5py.File(file, mode='r') as src:\n",
    "                images = src['matrix']\n",
    "                info = pd.read_hdf(file, key='info', mode='r')[['Vmax', 'time']]\n",
    "                slc = get_images_slice(images.shape, rotation_width)\n",
    "                file_len = images.shape[0]\n",
    "                for i in range(0, file_len, batch):\n",
    "                    batch_slc = slice(i, i + batch if i + batch < file_len else file_len)\n",
    "                    img_chunck = images[batch_slc, slc, slc, channels]\n",
    "                    info_chunck = info[batch_slc]\n",
    "                    img_chunck = clean_images(img_chunck)\n",
    "                    for j, (m, s) in enumerate(zip(means, stds)):\n",
    "                        img_chunck[:, :, :, j] -= m\n",
    "                        img_chunck[:, :, :, j] /= s\n",
    "\n",
    "                    img_new_shape = img_chunck.shape[1:]\n",
    "\n",
    "                    (train_img, train_info), (valid_img, valid_info) = split_data(img_chunck, info_chunck)\n",
    "\n",
    "                    if train_img.shape[0] > 0:\n",
    "                        if 'matrix' not in train:\n",
    "                            train.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        train['matrix'].resize(train['matrix'].shape[0] + train_img.shape[0], axis=0)\n",
    "                        train['matrix'][-train_img.shape[0]:] = train_img\n",
    "                        if 'info' not in train:\n",
    "                            train.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        train['info'].resize(train['info'].shape[0] + train_info.shape[0], axis=0)\n",
    "                        train['info'][-train_info.shape[0]:] = train_info\n",
    "\n",
    "\n",
    "                    if valid_img.shape[0] > 0:\n",
    "                        if 'matrix' not in valid:\n",
    "                            valid.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        valid['matrix'].resize(valid['matrix'].shape[0] + valid_img.shape[0], axis=0)\n",
    "                        valid['matrix'][-valid_img.shape[0]:] = valid_img\n",
    "                        if 'info' not in valid:\n",
    "                            valid.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        valid['info'].resize(valid['info'].shape[0] + valid_info.shape[0], axis=0)\n",
    "                        valid['info'][-valid_info.shape[0]:] = valid_info\n",
    "\n",
    "\n",
    "pre_process(ds, img_w, mean, std, batch=load_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d0e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:14.517571Z",
     "iopub.status.busy": "2025-05-19T18:40:14.517309Z",
     "iopub.status.idle": "2025-05-19T18:40:14.524877Z",
     "shell.execute_reply": "2025-05-19T18:40:14.524320Z",
     "shell.execute_reply.started": "2025-05-19T18:40:14.517548Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(train_path, mode='r') as trainsrc, h5py.File(valid_path, mode='r') as validsrc:\n",
    "    data_len = trainsrc['matrix'].shape[0]\n",
    "    valid_data_len = validsrc['matrix'].shape[0]\n",
    "\n",
    "    print('Dataset de treino: ', trainsrc['matrix'].shape)\n",
    "    print('Dataset de validação: ', validsrc['matrix'].shape)\n",
    "\n",
    "iter_train = data_len // batch\n",
    "iter_valid = valid_data_len // batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b06b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:14.525806Z",
     "iopub.status.busy": "2025-05-19T18:40:14.525547Z",
     "iopub.status.idle": "2025-05-19T18:40:14.548030Z",
     "shell.execute_reply": "2025-05-19T18:40:14.547450Z",
     "shell.execute_reply.started": "2025-05-19T18:40:14.525785Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_image_tf(image)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_image_tf(image):\n",
    "    angle_rad = tf.random.uniform([], 0, 2 * np.pi)\n",
    "    image_shape = tf.shape(image)[0:2]\n",
    "    cx = tf.cast(image_shape[1] / 2, tf.float32)\n",
    "    cy = tf.cast(image_shape[0] / 2, tf.float32)\n",
    "    cos_a = tf.math.cos(angle_rad)\n",
    "    sin_a = tf.math.sin(angle_rad)\n",
    "    transform = tf.stack([\n",
    "        cos_a, -sin_a, (1 - cos_a) * cx + sin_a * cy,\n",
    "        sin_a,  cos_a, (1 - cos_a) * cy - sin_a * cx,\n",
    "        0.0,    0.0\n",
    "    ])\n",
    "    transform = tf.reshape(transform, [8])\n",
    "    transform = tf.expand_dims(transform, 0)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=image,\n",
    "        transforms=transform,\n",
    "        output_shape=image_shape,\n",
    "        interpolation=\"BILINEAR\",\n",
    "        fill_mode=\"REFLECT\",\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    rotated = tf.squeeze(rotated, 0)\n",
    "    return tf.image.resize_with_crop_or_pad(rotated, img_w, img_w)\n",
    "\n",
    "def load_dataset(file, batch_size):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        images = f['matrix'][:]\n",
    "        labels = f['info'][:]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=len(images))\n",
    "    dataset = dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601472b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:14.548985Z",
     "iopub.status.busy": "2025-05-19T18:40:14.548662Z",
     "iopub.status.idle": "2025-05-19T18:40:14.563673Z",
     "shell.execute_reply": "2025-05-19T18:40:14.563088Z",
     "shell.execute_reply.started": "2025-05-19T18:40:14.548948Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    with h5py.File(file, mode='r') as src:\n",
    "        images = src['matrix'][:]\n",
    "        info = src['info'][:]\n",
    "        images = cut_images(images, img_w)\n",
    "        return tf.constant(images), tf.constant(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275db43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:14.564685Z",
     "iopub.status.busy": "2025-05-19T18:40:14.564458Z",
     "iopub.status.idle": "2025-05-19T18:40:57.428460Z",
     "shell.execute_reply": "2025-05-19T18:40:57.427593Z",
     "shell.execute_reply.started": "2025-05-19T18:40:14.564668Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_path, batch)\n",
    "valid_ds = get_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b9a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:57.429562Z",
     "iopub.status.busy": "2025-05-19T18:40:57.429338Z",
     "iopub.status.idle": "2025-05-19T18:40:57.437081Z",
     "shell.execute_reply": "2025-05-19T18:40:57.436367Z",
     "shell.execute_reply.started": "2025-05-19T18:40:57.429545Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_shape, strides=(2, 2)):\n",
    "    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "    reg = keras.regularizers.L2(1e-5)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "    model.add(keras.layers.Conv2D(16, (4, 4), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(1, activation='linear', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=.5e-4), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7a33a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:40:57.438206Z",
     "iopub.status.busy": "2025-05-19T18:40:57.437967Z",
     "iopub.status.idle": "2025-05-19T18:40:57.456555Z",
     "shell.execute_reply": "2025-05-19T18:40:57.455764Z",
     "shell.execute_reply.started": "2025-05-19T18:40:57.438189Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_number():\n",
    "    counter = 0\n",
    "    while True:\n",
    "        yield counter\n",
    "        counter += 1\n",
    "\n",
    "training_n_gen = training_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267335b0",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-19T23:05:57.973Z",
     "iopub.execute_input": "2025-05-19T18:40:57.457577Z",
     "iopub.status.busy": "2025-05-19T18:40:57.457338Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_n = next(training_n_gen)\n",
    "\n",
    "model = build_model((img_w, img_w, len(channels)),)\n",
    "model.summary()\n",
    "\n",
    "best_model_path = '{epoch:02d}-{val_loss:.2f}.keras'\n",
    "callback = ModelCheckpoint(filepath=best_model_path,\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=iter_train,\n",
    "        validation_steps=iter_valid,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "\n",
    "model_info_dict = {\n",
    "    \"seed\": seed,\n",
    "    \"shape\": [\n",
    "        img_w,\n",
    "        img_w,\n",
    "        len(channels)\n",
    "    ],\n",
    "    \"channels\": channels,\n",
    "    \"dataset\": ds,\n",
    "    \"batch\": batch,\n",
    "    \"normparams\": [{\"mean\": mean[i], \"std\": std[i]} for i in range(len(channels))],\n",
    "    \"validmse\": list(history.history['val_loss']),\n",
    "    \"trainingmse\": list(history.history['loss'])\n",
    "}\n",
    "\n",
    "json_info = json.dumps(model_info_dict, indent=4)\n",
    "with open(f'n{training_n}-model_info.json', 'w') as outfile:\n",
    "    outfile.write(json_info)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7339781,
     "sourceId": 11694029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7379404,
     "sourceId": 11754607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7224465,
     "sourceId": 11830382,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-20T01:17:11.370598",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
