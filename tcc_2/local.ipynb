{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:18.627229Z",
     "iopub.status.busy": "2026-02-04T16:10:18.627019Z",
     "iopub.status.idle": "2026-02-04T16:10:31.339759Z",
     "shell.execute_reply": "2026-02-04T16:10:31.339207Z",
     "shell.execute_reply.started": "2026-02-04T16:10:18.627212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../tcc_2/train.h5\", mode=\"r\") as f:\n",
    "    imgs = f[\"matrix\"]\n",
    "    print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.405177Z",
     "iopub.status.busy": "2026-02-04T16:10:32.404821Z",
     "iopub.status.idle": "2026-02-04T16:10:32.412167Z",
     "shell.execute_reply": "2026-02-04T16:10:32.411353Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.405147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# seed = 1747265027\n",
    "seed = int(time.time()) % (2**32 - 1)  # ou: random.randint(0, 999999)\n",
    "print(f\"Usando seed: {seed}\")\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.413390Z",
     "iopub.status.busy": "2026-02-04T16:10:32.413106Z",
     "iopub.status.idle": "2026-02-04T16:10:32.425531Z",
     "shell.execute_reply": "2026-02-04T16:10:32.424921Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.413365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_path, valid_path, test_path = 'train.h5', 'valid.h5', 'test.h5'\n",
    "ds = [f'../data/{i}' for i in ['TCIR-ATLN_EPAC_WPAC.h5', 'TCIR-CPAC_IO_SH.h5']]\n",
    "channels = [0, 3]\n",
    "generated_channels = [0]\n",
    "img_w = 64\n",
    "rotation_width = None\n",
    "load_batch = 4096 * 2\n",
    "epochs = 700\n",
    "batch = 8\n",
    "lr = .2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.426397Z",
     "iopub.status.busy": "2026-02-04T16:10:32.426181Z",
     "iopub.status.idle": "2026-02-04T16:10:32.436033Z",
     "shell.execute_reply": "2026-02-04T16:10:32.435406Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.426381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_images_slice(images_shape, width):\n",
    "    start = images_shape[1] // 2 - width // 2\n",
    "    end = images_shape[1] // 2 + width // 2\n",
    "    return slice(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.437601Z",
     "iopub.status.busy": "2026-02-04T16:10:32.436714Z",
     "iopub.status.idle": "2026-02-04T16:10:32.445825Z",
     "shell.execute_reply": "2026-02-04T16:10:32.445338Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.437583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cut_images(images, width):\n",
    "    slc = get_images_slice(images.shape, width)\n",
    "    return images[:, slc, slc, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.447143Z",
     "iopub.status.busy": "2026-02-04T16:10:32.446444Z",
     "iopub.status.idle": "2026-02-04T16:10:32.457074Z",
     "shell.execute_reply": "2026-02-04T16:10:32.456296Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.447121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_images(images):\n",
    "    images = np.nan_to_num(images, copy=False)\n",
    "    images[images > 1000] = 0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.458273Z",
     "iopub.status.busy": "2026-02-04T16:10:32.458016Z",
     "iopub.status.idle": "2026-02-04T16:10:32.467313Z",
     "shell.execute_reply": "2026-02-04T16:10:32.466600Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.458252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_data(images, info):\n",
    "    years = [datetime.datetime.strptime(i, \"%Y%m%d%H\").year for i in list(info['time'])]\n",
    "    years = np.array(years)\n",
    "    train_values = (years >= 2003) & (years <= 2014)\n",
    "    valid_values = (years >= 2015) & (years <= 2016)\n",
    "    train_idx = np.where(train_values)[0]\n",
    "    valid_idx = np.where(valid_values)[0]\n",
    "    info = info['Vmax'].to_numpy()\n",
    "    train_img, train_info = images[train_idx], info[train_idx]\n",
    "    valid_img, valid_info = images[valid_idx], info[valid_idx]\n",
    "    return (train_img, train_info), (valid_img, valid_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:10:32.470632Z",
     "iopub.status.busy": "2026-02-04T16:10:32.470433Z",
     "iopub.status.idle": "2026-02-04T16:18:02.000680Z",
     "shell.execute_reply": "2026-02-04T16:18:02.000009Z",
     "shell.execute_reply.started": "2026-02-04T16:10:32.470618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_mean(files, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum(train_image[:, :, :, j])\n",
    "    means = accumulators / (files_data_len * width * width)\n",
    "    return means\n",
    "\n",
    "mean = get_mean(ds, batch=load_batch, width=img_w)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:18:02.001732Z",
     "iopub.status.busy": "2026-02-04T16:18:02.001473Z",
     "iopub.status.idle": "2026-02-04T16:25:40.903747Z",
     "shell.execute_reply": "2026-02-04T16:25:40.902887Z",
     "shell.execute_reply.started": "2026-02-04T16:18:02.001708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_std(files, mean, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum((train_image[:, :, :, j] - mean[j]) ** 2)\n",
    "    stds = accumulators / (files_data_len * width * width)\n",
    "    stds = np.sqrt(stds)\n",
    "    return stds\n",
    "\n",
    "std = get_std(ds, mean, batch=load_batch, width=img_w)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:25:40.904852Z",
     "iopub.status.busy": "2026-02-04T16:25:40.904598Z",
     "iopub.status.idle": "2026-02-04T16:25:48.621913Z",
     "shell.execute_reply": "2026-02-04T16:25:48.621047Z",
     "shell.execute_reply.started": "2026-02-04T16:25:40.904827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "indexes = []\n",
    "for i, d in enumerate(ds):\n",
    "    info = pd.read_hdf(d, key='info', mode='r')[['ID', 'Vmax', 'time']]\n",
    "    ids.append(list(info['ID'].unique()))\n",
    "    # indexes.append([info.index[info['ID'] == ide].sort(key=lambda val: datetime.datetime.strptime(val, \"%Y%m%d%H\")) for ide in ids[i]])\n",
    "    sorted_indexes = []\n",
    "    for ide in ids[i]:\n",
    "        # Filtra o DataFrame para o ID atual\n",
    "        sub_info = info[info['ID'] == ide]\n",
    "        \n",
    "        # Converte a coluna 'time' para datetime\n",
    "        sub_info = sub_info.copy()\n",
    "        sub_info['time_dt'] = sub_info['time'].apply(lambda t: datetime.datetime.strptime(str(t), \"%Y%m%d%H\"))\n",
    "        \n",
    "        # Ordena pelo datetime e pega os índices ordenados\n",
    "        sorted_idx = sub_info.sort_values('time_dt').index.tolist()\n",
    "        sorted_indexes.append(sorted_idx)\n",
    "    \n",
    "    indexes.append(sorted_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:25:48.623079Z",
     "iopub.status.busy": "2026-02-04T16:25:48.622834Z",
     "iopub.status.idle": "2026-02-04T16:25:48.924796Z",
     "shell.execute_reply": "2026-02-04T16:25:48.924081Z",
     "shell.execute_reply.started": "2026-02-04T16:25:48.623063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def compute_optical_flow(img1, img2):\n",
    "    img1 = img1.astype(np.float32)\n",
    "    img2 = img2.astype(np.float32)\n",
    "\n",
    "    # Optical flow usando Farneback (rápido e bom para deep learning)\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        img1, img2,\n",
    "        None,\n",
    "        pyr_scale=0.5,\n",
    "        levels=3,\n",
    "        winsize=15,\n",
    "        iterations=3,\n",
    "        poly_n=5,\n",
    "        poly_sigma=1.2,\n",
    "        flags=0\n",
    "    )\n",
    "\n",
    "    # flow tem shape (H, W, 2): componente horizontal e vertical\n",
    "    u = flow[:, :, 0]\n",
    "    v = flow[:, :, 1]\n",
    "\n",
    "    # magnitude e direção\n",
    "    # magnitude = np.sqrt(u**2 + v**2)\n",
    "    # direction = np.arctan2(v, u)\n",
    "\n",
    "    # return magnitude, direction\n",
    "    return u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:25:48.925727Z",
     "iopub.status.busy": "2026-02-04T16:25:48.925531Z",
     "iopub.status.idle": "2026-02-04T16:36:27.714479Z",
     "shell.execute_reply": "2026-02-04T16:36:27.713878Z",
     "shell.execute_reply.started": "2026-02-04T16:25:48.925712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pre_process(files, width, means, stds, batch=1024):\n",
    "    # corta imagem grande o suficiente para poder rotacionar\n",
    "    global rotation_width\n",
    "    rotation_width = int(np.ceil(np.sqrt((width ** 2) * 2)))\n",
    "    if rotation_width % 2 != 0:\n",
    "        rotation_width += 1\n",
    "\n",
    "    files_data_len = np.zeros(len(files))\n",
    "    with h5py.File(train_path, 'w') as train, h5py.File(valid_path, 'w') as valid:\n",
    "        for fi, file in enumerate(files): # itera sobre os arquivos\n",
    "            file_cyclone_ids = ids[fi] #ids dos ciclones no arquivo\n",
    "            file_cyclone_indexes = indexes[fi] # índices dos ciclones no arquivo\n",
    "            with h5py.File(file, mode='r') as src:\n",
    "                images = src['matrix']\n",
    "                info = pd.read_hdf(file, key='info', mode='r')[['ID', 'Vmax', 'time']]\n",
    "                slc = get_images_slice(images.shape, rotation_width)\n",
    "                file_len = images.shape[0]\n",
    "                for id_idx, cyclone_id in enumerate(file_cyclone_ids): # itera os ciclones (índice e ID)\n",
    "                    cy_idxs = list(file_cyclone_indexes[id_idx]) # lista de índices das imagens de um ciclone\n",
    "                    cyclone_images = images[cy_idxs, slc, slc]\n",
    "                    cyclone_images = cyclone_images[:, :, :, channels]\n",
    "                    cyclone_info = info.iloc[cy_idxs]\n",
    "                    \n",
    "                    cyclone_images = clean_images(cyclone_images)\n",
    "                    for j, (m, s) in enumerate(zip(means, stds)):\n",
    "                        cyclone_images[:, :, :, j] -= m\n",
    "                        cyclone_images[:, :, :, j] /= s\n",
    "\n",
    "                    # generated_channels_idx = [channels.index(ch) for ch in generated_channels]\n",
    "\n",
    "                    new_channels = []\n",
    "                    for j in range(1, cyclone_images.shape[0]):\n",
    "                        current_img = np.expand_dims(cyclone_images[j, :, :, generated_channels][0], axis=-1)\n",
    "                        previous_img = np.expand_dims(cyclone_images[j - 1, :, :, generated_channels][0], axis=-1)\n",
    "                        magnitude, direction = compute_optical_flow(current_img, previous_img)\n",
    "                        magnitude = np.expand_dims(magnitude, axis=-1)\n",
    "                        direction = np.expand_dims(direction, axis=-1)\n",
    "                        conc = np.concatenate((magnitude, direction), axis=-1)\n",
    "                        new_channels.append(conc)\n",
    "                    new_channels = np.array(new_channels)\n",
    "\n",
    "                    cyclone_images = cyclone_images[:-1]\n",
    "                    cyclone_info = cyclone_info[:-1]\n",
    "                    cyclone_images = np.concatenate((cyclone_images, new_channels), axis=-1)\n",
    "                    img_new_shape = cyclone_images.shape[1:]\n",
    "\n",
    "                    (train_img, train_info), (valid_img, valid_info) = split_data(cyclone_images, cyclone_info)\n",
    "\n",
    "                    if train_img.shape[0] > 0:\n",
    "                        if 'matrix' not in train:\n",
    "                            train.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        train['matrix'].resize(train['matrix'].shape[0] + train_img.shape[0], axis=0)\n",
    "                        train['matrix'][-train_img.shape[0]:] = train_img\n",
    "                        if 'info' not in train:\n",
    "                            train.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        train['info'].resize(train['info'].shape[0] + train_info.shape[0], axis=0)\n",
    "                        train['info'][-train_info.shape[0]:] = train_info\n",
    "\n",
    "\n",
    "                    if valid_img.shape[0] > 0:\n",
    "                        if 'matrix' not in valid:\n",
    "                            valid.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        valid['matrix'].resize(valid['matrix'].shape[0] + valid_img.shape[0], axis=0)\n",
    "                        valid['matrix'][-valid_img.shape[0]:] = valid_img\n",
    "                        if 'info' not in valid:\n",
    "                            valid.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        valid['info'].resize(valid['info'].shape[0] + valid_info.shape[0], axis=0)\n",
    "                        valid['info'][-valid_info.shape[0]:] = valid_info\n",
    "\n",
    "import os\n",
    "if not os.path.exists(train_path) or not os.path.exists(valid_path):\n",
    "    pre_process(ds, img_w, mean, std, batch=load_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:36:27.715452Z",
     "iopub.status.busy": "2026-02-04T16:36:27.715218Z",
     "iopub.status.idle": "2026-02-04T16:36:27.940729Z",
     "shell.execute_reply": "2026-02-04T16:36:27.940128Z",
     "shell.execute_reply.started": "2026-02-04T16:36:27.715435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(train_path, mode='r') as f:\n",
    "    img = f['matrix'][100, :, :, 2]\n",
    "    plt.imshow(img)\n",
    "    print(f['matrix'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T16:36:27.941558Z",
     "iopub.status.busy": "2026-02-04T16:36:27.941367Z",
     "iopub.status.idle": "2026-02-04T16:36:27.948141Z",
     "shell.execute_reply": "2026-02-04T16:36:27.947472Z",
     "shell.execute_reply.started": "2026-02-04T16:36:27.941545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(train_path, mode='r') as trainsrc, h5py.File(valid_path, mode='r') as validsrc:\n",
    "    data_len = trainsrc['matrix'].shape[0]\n",
    "    valid_data_len = validsrc['matrix'].shape[0]\n",
    "\n",
    "    print('Dataset de treino: ', trainsrc['matrix'].shape)\n",
    "    print('Dataset de validação: ', validsrc['matrix'].shape)\n",
    "\n",
    "iter_train = data_len // batch\n",
    "iter_valid = valid_data_len // batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:29.255628Z",
     "iopub.status.busy": "2026-02-04T17:41:29.254878Z",
     "iopub.status.idle": "2026-02-04T17:41:29.264120Z",
     "shell.execute_reply": "2026-02-04T17:41:29.263461Z",
     "shell.execute_reply.started": "2026-02-04T17:41:29.255603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_image_tf(image)\n",
    "\n",
    "    # image.set_shape((64, 64, 4))\n",
    "    # label.set_shape((1,))\n",
    "\n",
    "    print(label)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def preprocess_image_tf(image):\n",
    "    angle_rad = tf.random.uniform([], 0, 2 * np.pi)\n",
    "    image_shape = tf.shape(image)[0:2]\n",
    "    cx = tf.cast(image_shape[1] / 2, tf.float32)\n",
    "    cy = tf.cast(image_shape[0] / 2, tf.float32)\n",
    "    cos_a = tf.math.cos(angle_rad)\n",
    "    sin_a = tf.math.sin(angle_rad)\n",
    "    transform = tf.stack([\n",
    "        cos_a, -sin_a, (1 - cos_a) * cx + sin_a * cy,\n",
    "        sin_a,  cos_a, (1 - cos_a) * cy - sin_a * cx,\n",
    "        0.0,    0.0\n",
    "    ])\n",
    "    transform = tf.reshape(transform, [8])\n",
    "    transform = tf.expand_dims(transform, 0)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=image,\n",
    "        transforms=transform,\n",
    "        output_shape=image_shape,\n",
    "        interpolation=\"BILINEAR\",\n",
    "        fill_mode=\"REFLECT\",\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    rotated = tf.squeeze(rotated, 0)\n",
    "    return tf.image.resize_with_crop_or_pad(rotated, img_w, img_w)\n",
    "\n",
    "def load_dataset(file, batch_size, shuffle_buffer=1024):\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        images = f['matrix'][:]\n",
    "        labels = f['info'][:]\n",
    "\n",
    "    def batch_generator():\n",
    "        for i in range(0, images.shape[0], load_batch):\n",
    "            yield (images[i: i + load_batch], labels[i: i + load_batch])\n",
    "    \n",
    "    def generator():\n",
    "        batch_gen = batch_generator()\n",
    "        while True:\n",
    "            try:\n",
    "                batch_images, batch_labels = next(batch_gen)\n",
    "            except StopIteration:\n",
    "                batch_gen = batch_generator()\n",
    "            for img, lbl in zip(batch_images, batch_labels): \n",
    "                yield img, lbl\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(rotation_width, rotation_width, len(channels) + len(generated_channels) * 2)),\n",
    "            tf.TensorSpec(shape=()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = tf.data.Dataset.from_tensor_slices([])\n",
    "# # with h5py.File(file, 'r') as f:\n",
    "# #     images = f['matrix'][:]\n",
    "# #     labels = f['info'][:]\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# load_batch = 5\n",
    "# ls = np.linspace(0.0, 10.0, 10)\n",
    "# def gen():\n",
    "#     for i in ls:\n",
    "#         # yield (tf.constant(ls[i: i + load_batch]),)\n",
    "#         yield (tf.constant(i),)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_generator(\n",
    "#     gen,\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# dataset = dataset.repeat(2)\n",
    "# dataset = dataset.shuffle(buffer_size=load_batch)\n",
    "\n",
    "# # dataset = dataset.repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = list(dataset)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:31.835351Z",
     "iopub.status.busy": "2026-02-04T17:41:31.835050Z",
     "iopub.status.idle": "2026-02-04T17:41:31.840212Z",
     "shell.execute_reply": "2026-02-04T17:41:31.839568Z",
     "shell.execute_reply.started": "2026-02-04T17:41:31.835328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    with h5py.File(file, mode='r') as src:\n",
    "        images = src['matrix'][:]\n",
    "        info = src['info'][:]\n",
    "        images = cut_images(images, img_w)\n",
    "        return tf.constant(images), tf.constant(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:33.079533Z",
     "iopub.status.busy": "2026-02-04T17:41:33.078769Z",
     "iopub.status.idle": "2026-02-04T17:41:36.494061Z",
     "shell.execute_reply": "2026-02-04T17:41:36.493274Z",
     "shell.execute_reply.started": "2026-02-04T17:41:33.079506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_path, batch)\n",
    "valid_ds = get_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:41.196019Z",
     "iopub.status.busy": "2026-02-04T17:41:41.195720Z",
     "iopub.status.idle": "2026-02-04T17:41:41.203625Z",
     "shell.execute_reply": "2026-02-04T17:41:41.202844Z",
     "shell.execute_reply.started": "2026-02-04T17:41:41.195989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_shape, strides=(2, 2)):\n",
    "    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "    reg = keras.regularizers.L2(5e-3)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "    model.add(keras.layers.Conv2D(16, (4, 4), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(1, activation='linear', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:43.326540Z",
     "iopub.status.busy": "2026-02-04T17:41:43.325866Z",
     "iopub.status.idle": "2026-02-04T17:41:43.330159Z",
     "shell.execute_reply": "2026-02-04T17:41:43.329338Z",
     "shell.execute_reply.started": "2026-02-04T17:41:43.326514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_number():\n",
    "    counter = 0\n",
    "    while True:\n",
    "        yield counter\n",
    "        counter += 1\n",
    "\n",
    "training_n_gen = training_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T17:41:45.369518Z",
     "iopub.status.busy": "2026-02-04T17:41:45.368785Z",
     "iopub.status.idle": "2026-02-04T17:41:45.611453Z",
     "shell.execute_reply": "2026-02-04T17:41:45.610342Z",
     "shell.execute_reply.started": "2026-02-04T17:41:45.369491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_n = next(training_n_gen)\n",
    "\n",
    "\n",
    "model = build_model((img_w, img_w, len(channels) + len(generated_channels) * 2),)\n",
    "model.summary()\n",
    "\n",
    "best_model_path = '{epoch:03d}-{loss:.2f}_{val_loss:.2f}.keras'\n",
    "callback = ModelCheckpoint(filepath=best_model_path,\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=False,\n",
    "                           mode='min')\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=iter_train,\n",
    "        validation_steps=iter_valid,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "\n",
    "model_info_dict = {\n",
    "    \"seed\": seed,\n",
    "    \"shape\": [\n",
    "        img_w,\n",
    "        img_w,\n",
    "        len(channels)\n",
    "    ],\n",
    "    \"channels\": channels,\n",
    "    \"dataset\": ds,\n",
    "    \"batch\": batch,\n",
    "    \"normparams\": [{\"mean\": mean[i], \"std\": std[i]} for i in range(len(channels))],\n",
    "    \"validmse\": list(history.history['val_loss']),\n",
    "    \"trainingmse\": list(history.history['loss'])\n",
    "}\n",
    "\n",
    "json_info = json.dumps(model_info_dict, indent=4)\n",
    "with open(f'n{training_n}-model_info.json', 'w') as outfile:\n",
    "    outfile.write(json_info)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7224465,
     "sourceId": 11830382,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7523074,
     "sourceId": 11964049,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
