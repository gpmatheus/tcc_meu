{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "# from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# seed = 1747265027\n",
    "seed = int(time.time()) % (2**32 - 1)  # ou: random.randint(0, 999999)\n",
    "print(f\"Usando seed: {seed}\")\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_path, valid_path, test_path = 'train.h5', 'valid.h5', 'test.h5'\n",
    "ds = [f'{i}' for i in ['TCIR-ATLN_EPAC_WPAC.h5', 'TCIR-CPAC_IO_SH.h5']]\n",
    "channels = [0, 3]\n",
    "generated_channels = [0]\n",
    "img_w = 64\n",
    "load_batch = 4096\n",
    "epochs = 500\n",
    "batch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_images_slice(images_shape, width):\n",
    "    start = images_shape[1] // 2 - width // 2\n",
    "    end = images_shape[1] // 2 + width // 2\n",
    "    return slice(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cut_images(images, width):\n",
    "    slc = get_images_slice(images.shape, width)\n",
    "    return images[:, slc, slc, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_images(images):\n",
    "    images = np.nan_to_num(images, copy=False)\n",
    "    images[images > 1000] = 0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_data(images, info):\n",
    "    years = [datetime.datetime.strptime(i, \"%Y%m%d%H\").year for i in list(info['time'])]\n",
    "    years = np.array(years)\n",
    "    train_values = (years >= 2003) & (years <= 2014)\n",
    "    valid_values = (years >= 2015) & (years <= 2016)\n",
    "    train_idx = np.where(train_values)[0]\n",
    "    valid_idx = np.where(valid_values)[0]\n",
    "    info = info['Vmax'].to_numpy()\n",
    "    train_img, train_info = images[train_idx], info[train_idx]\n",
    "    valid_img, valid_info = images[valid_idx], info[valid_idx]\n",
    "    return (train_img, train_info), (valid_img, valid_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_mean(files, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum(train_image[:, :, :, j])\n",
    "    means = accumulators / (files_data_len * width * width)\n",
    "    return means\n",
    "\n",
    "mean = get_mean(ds, batch=load_batch, width=img_w)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_std(files, mean, batch=1024, width=64):\n",
    "    accumulators = np.zeros(len(channels))\n",
    "    files_data_len = 0.0\n",
    "    for fi, file in enumerate(files):\n",
    "        with h5py.File(file, mode='r') as src:\n",
    "            images = src['matrix']\n",
    "            info = pd.read_hdf(file, key='info', mode='r')\n",
    "            slc = get_images_slice(images.shape, width)\n",
    "            file_len = images.shape[0]\n",
    "            for i in range(0, file_len, batch):\n",
    "                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n",
    "                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n",
    "                image_chunck = clean_images(image_chunck)\n",
    "                (train_image, _), _ = split_data(image_chunck, info_chunck)\n",
    "                files_data_len += train_image.shape[0]\n",
    "                for j in range(accumulators.shape[0]):\n",
    "                    accumulators[j] += np.sum((train_image[:, :, :, j] - mean[j]) ** 2)\n",
    "    stds = accumulators / (files_data_len * width * width)\n",
    "    stds = np.sqrt(stds)\n",
    "    return stds\n",
    "\n",
    "std = get_std(ds, mean, batch=load_batch, width=img_w)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "indexes = []\n",
    "for i, d in enumerate(ds):\n",
    "    info = pd.read_hdf(d, key='info', mode='r')[['ID', 'Vmax', 'time']]\n",
    "    ids.append(list(info['ID'].unique()))\n",
    "    print(ids[i])\n",
    "    # indexes.append([info.index[info['ID'] == ide].sort(key=lambda val: datetime.datetime.strptime(val, \"%Y%m%d%H\")) for ide in ids[i]])\n",
    "    sorted_indexes = []\n",
    "    for ide in ids[i]:\n",
    "        # Filtra o DataFrame para o ID atual\n",
    "        sub_info = info[info['ID'] == ide]\n",
    "        \n",
    "        # Converte a coluna 'time' para datetime\n",
    "        sub_info = sub_info.copy()\n",
    "        sub_info['time_dt'] = sub_info['time'].apply(lambda t: datetime.datetime.strptime(str(t), \"%Y%m%d%H\"))\n",
    "        \n",
    "        # Ordena pelo datetime e pega os Ã­ndices ordenados\n",
    "        sorted_idx = sub_info.sort_values('time_dt').index.tolist()\n",
    "        sorted_indexes.append(sorted_idx)\n",
    "    \n",
    "    indexes.append(sorted_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pre_process(files, width, means, stds, batch=1024):\n",
    "    # corta imagem grande o suficiente para poder rotacionar\n",
    "    rotation_width = int(np.ceil(np.sqrt((width ** 2) * 2)))\n",
    "    if rotation_width % 2 != 0:\n",
    "        rotation_width += 1\n",
    "\n",
    "    files_data_len = np.zeros(len(files))\n",
    "    with h5py.File(train_path, 'w') as train, h5py.File(valid_path, 'w') as valid:\n",
    "        for fi, file in enumerate(files): # itera sobre os arquivos\n",
    "            file_cyclone_ids = ids[fi] #ids dos ciclones no arquivo\n",
    "            file_cyclone_indexes = indexes[fi] # Ã­ndices dos ciclones no arquivo\n",
    "            with h5py.File(file, mode='r') as src:\n",
    "                images = src['matrix']\n",
    "                info = pd.read_hdf(file, key='info', mode='r')[['ID', 'Vmax', 'time']]\n",
    "                slc = get_images_slice(images.shape, rotation_width)\n",
    "                file_len = images.shape[0]\n",
    "                for id_idx, cyclone_id in enumerate(file_cyclone_ids): # itera os ciclones (Ã­ndice e ID)\n",
    "                    cy_idxs = list(file_cyclone_indexes[id_idx]) # lista de Ã­ndices das imagens de um ciclone\n",
    "                    cyclone_images = images[cy_idxs, slc, slc]\n",
    "                    cyclone_images = cyclone_images[:, :, :, channels]\n",
    "                    cyclone_info = info.iloc[cy_idxs]\n",
    "                    \n",
    "                    cyclone_images = clean_images(cyclone_images)\n",
    "                    for j, (m, s) in enumerate(zip(means, stds)):\n",
    "                        cyclone_images[:, :, :, j] -= m\n",
    "                        cyclone_images[:, :, :, j] /= s\n",
    "\n",
    "                    generated_channels_idx = [channels.index(ch) for ch in generated_channels]\n",
    "\n",
    "                    new_channels = []\n",
    "                    for j in range(cyclone_images.shape[0] - 1):\n",
    "                        current_img = np.expand_dims(cyclone_images[j, :, :, generated_channels_idx][0], axis=-1)\n",
    "                        next_img = np.expand_dims(cyclone_images[j + 1, :, :, generated_channels_idx][0], axis=-1)\n",
    "                        new_ch = current_img - next_img\n",
    "                        print(current_img.shape, next_img.shape, new_ch.shape)\n",
    "                        # new_ch = new_ch.reshape(cyclone_images.shape[1:-1] + (len(generated_channels_idx),))\n",
    "                        new_channels.append(new_ch)\n",
    "                    new_channels = np.array(new_channels)\n",
    "\n",
    "                    # new_channels = np.array([(cyclone_images[j, :, :, generated_channels_idx] - cyclone_images[j + 1, :, :, generated_channels_idx]).reshape(cyclone_images.shape[:-1] + (len(generated_channels_idx),)) for j in range(cyclone_images.shape[0] - 1)])\n",
    "                    # cyclone_images = np.resize(cyclone_images, cyclone_images.shape[:-1] + (cyclone_images.shape[-1] + new_channels.shape[-1],))\n",
    "                    cyclone_images = cyclone_images[:-1]\n",
    "                    cyclone_info = cyclone_info[:-1]\n",
    "                    cyclone_images = np.concatenate((cyclone_images, new_channels), axis=-1)\n",
    "                    img_new_shape = cyclone_images.shape[1:]\n",
    "                    print(new_channels.shape)\n",
    "                    # for j in range(new_channels.shape[-1]):\n",
    "                    #     cyclone_images[:, :, :, j + len(channels)] = new_channels[:, :, :, j]\n",
    "\n",
    "                    (train_img, train_info), (valid_img, valid_info) = split_data(cyclone_images, cyclone_info)\n",
    "\n",
    "                    if train_img.shape[0] > 0:\n",
    "                        if 'matrix' not in train:\n",
    "                            train.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        train['matrix'].resize(train['matrix'].shape[0] + train_img.shape[0], axis=0)\n",
    "                        train['matrix'][-train_img.shape[0]:] = train_img\n",
    "                        if 'info' not in train:\n",
    "                            train.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        train['info'].resize(train['info'].shape[0] + train_info.shape[0], axis=0)\n",
    "                        train['info'][-train_info.shape[0]:] = train_info\n",
    "\n",
    "\n",
    "                    if valid_img.shape[0] > 0:\n",
    "                        if 'matrix' not in valid:\n",
    "                            valid.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n",
    "                        valid['matrix'].resize(valid['matrix'].shape[0] + valid_img.shape[0], axis=0)\n",
    "                        valid['matrix'][-valid_img.shape[0]:] = valid_img\n",
    "                        if 'info' not in valid:\n",
    "                            valid.create_dataset('info', shape=(0,), maxshape=(None,))\n",
    "                        valid['info'].resize(valid['info'].shape[0] + valid_info.shape[0], axis=0)\n",
    "                        valid['info'][-valid_info.shape[0]:] = valid_info\n",
    "\n",
    "\n",
    "pre_process(ds, img_w, mean, std, batch=load_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(train_path, mode='r') as f:\n",
    "    img = f['matrix'][100, :, :, 0]\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(train_path, mode='r') as trainsrc, h5py.File(valid_path, mode='r') as validsrc:\n",
    "    data_len = trainsrc['matrix'].shape[0]\n",
    "    valid_data_len = validsrc['matrix'].shape[0]\n",
    "\n",
    "    print('Dataset de treino: ', trainsrc['matrix'].shape)\n",
    "    print('Dataset de validaÃ§Ã£o: ', validsrc['matrix'].shape)\n",
    "\n",
    "iter_train = data_len // batch\n",
    "iter_valid = valid_data_len // batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_image_tf(image)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_image_tf(image):\n",
    "    angle_rad = tf.random.uniform([], 0, 2 * np.pi)\n",
    "    image_shape = tf.shape(image)[0:2]\n",
    "    cx = tf.cast(image_shape[1] / 2, tf.float32)\n",
    "    cy = tf.cast(image_shape[0] / 2, tf.float32)\n",
    "    cos_a = tf.math.cos(angle_rad)\n",
    "    sin_a = tf.math.sin(angle_rad)\n",
    "    transform = tf.stack([\n",
    "        cos_a, -sin_a, (1 - cos_a) * cx + sin_a * cy,\n",
    "        sin_a,  cos_a, (1 - cos_a) * cy - sin_a * cx,\n",
    "        0.0,    0.0\n",
    "    ])\n",
    "    transform = tf.reshape(transform, [8])\n",
    "    transform = tf.expand_dims(transform, 0)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=image,\n",
    "        transforms=transform,\n",
    "        output_shape=image_shape,\n",
    "        interpolation=\"BILINEAR\",\n",
    "        fill_mode=\"REFLECT\",\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    rotated = tf.squeeze(rotated, 0)\n",
    "    return tf.image.resize_with_crop_or_pad(rotated, img_w, img_w)\n",
    "\n",
    "def load_dataset(file, batch_size):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        images = f['matrix'][:]\n",
    "        labels = f['info'][:]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=len(images))\n",
    "    dataset = dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    with h5py.File(file, mode='r') as src:\n",
    "        images = src['matrix'][:]\n",
    "        info = src['info'][:]\n",
    "        images = cut_images(images, img_w)\n",
    "        return tf.constant(images), tf.constant(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_path, batch)\n",
    "# valid_ds = load_dataset(processed_valid, batch)\n",
    "valid_ds = get_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_shape, strides=(2, 2)):\n",
    "    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "    reg = keras.regularizers.L2(1e-5)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "    model.add(keras.layers.Conv2D(16, (4, 4), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(1, activation='linear', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=.5e-4), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_number():\n",
    "    counter = 0\n",
    "    while True:\n",
    "        yield counter\n",
    "        counter += 1\n",
    "\n",
    "training_n_gen = training_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_n = next(training_n_gen)\n",
    "\n",
    "model = build_model((img_w, img_w, len(channels)),)\n",
    "model.summary()\n",
    "\n",
    "best_model_path = '{epoch:03d}-{val_loss:.2f}.keras'\n",
    "callback = ModelCheckpoint(filepath=best_model_path,\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=iter_train,\n",
    "        validation_steps=iter_valid,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "\n",
    "model_info_dict = {\n",
    "    \"seed\": seed,\n",
    "    \"shape\": [\n",
    "        img_w,\n",
    "        img_w,\n",
    "        len(channels)\n",
    "    ],\n",
    "    \"channels\": channels,\n",
    "    \"dataset\": ds,\n",
    "    \"batch\": batch,\n",
    "    \"normparams\": [{\"mean\": mean[i], \"std\": std[i]} for i in range(len(channels))],\n",
    "    \"validmse\": list(history.history['val_loss']),\n",
    "    \"trainingmse\": list(history.history['loss'])\n",
    "}\n",
    "\n",
    "json_info = json.dumps(model_info_dict, indent=4)\n",
    "with open(f'n{training_n}-model_info.json', 'w') as outfile:\n",
    "    outfile.write(json_info)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7224465,
     "sourceId": 11830382,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7523074,
     "sourceId": 11964049,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
