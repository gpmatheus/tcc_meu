{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11830382,"sourceType":"datasetVersion","datasetId":7224465},{"sourceId":11964049,"sourceType":"datasetVersion","datasetId":7523074}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport h5py\nimport time\nimport datetime\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json\nfrom tensorflow import keras\nfrom keras.callbacks import ModelCheckpoint","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# seed = 1747265027\nseed = int(time.time()) % (2**32 - 1)  # ou: random.randint(0, 999999)\nprint(f\"Usando seed: {seed}\")\n\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.config.threading.set_inter_op_parallelism_threads(1)\ntf.config.threading.set_intra_op_parallelism_threads(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path, valid_path, test_path = '/kaggle/working/train.h5', '/kaggle/working/valid.h5', '/kaggle/working/test.h5'\nds = [f'/kaggle/input/tcir-atln-epac-wpac-h5/{i}' for i in ['TCIR-ATLN_EPAC_WPAC.h5', 'TCIR-CPAC_IO_SH.h5']]\nchannels = [0, 3]\nimg_w = 64\nload_batch = 4096\nepochs = 500\nbatch = 8","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images_slice(images_shape, width):\n    start = images_shape[1] // 2 - width // 2\n    end = images_shape[1] // 2 + width // 2\n    return slice(start, end)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cut_images(images, width):\n    slc = get_images_slice(images.shape, width)\n    return images[:, slc, slc, :]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_images(images):\n    images = np.nan_to_num(images, copy=False)\n    images[images > 1000] = 0\n    return images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_data(images, info):\n    years = [datetime.datetime.strptime(i, \"%Y%m%d%H\").year for i in list(info['time'])]\n    years = np.array(years)\n    train_values = (years >= 2003) & (years <= 2014)\n    valid_values = (years >= 2015) & (years <= 2016)\n    train_idx = np.where(train_values)[0]\n    valid_idx = np.where(valid_values)[0]\n    info = info['Vmax'].to_numpy()\n    train_img, train_info = images[train_idx], info[train_idx]\n    valid_img, valid_info = images[valid_idx], info[valid_idx]\n    return (train_img, train_info), (valid_img, valid_info)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_mean(files, batch=1024, width=64):\n    accumulators = np.zeros(len(channels))\n    files_data_len = 0.0\n    for fi, file in enumerate(files):\n        with h5py.File(file, mode='r') as src:\n            images = src['matrix']\n            info = pd.read_hdf(file, key='info', mode='r')\n            slc = get_images_slice(images.shape, width)\n            file_len = images.shape[0]\n            for i in range(0, file_len, batch):\n                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n                image_chunck = clean_images(image_chunck)\n                (train_image, _), _ = split_data(image_chunck, info_chunck)\n                files_data_len += train_image.shape[0]\n                for j in range(accumulators.shape[0]):\n                    accumulators[j] += np.sum(train_image[:, :, :, j])\n    means = accumulators / (files_data_len * width * width)\n    return means\n\nmean = get_mean(ds, batch=load_batch, width=img_w)\nprint(mean)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_std(files, mean, batch=1024, width=64):\n    accumulators = np.zeros(len(channels))\n    files_data_len = 0.0\n    for fi, file in enumerate(files):\n        with h5py.File(file, mode='r') as src:\n            images = src['matrix']\n            info = pd.read_hdf(file, key='info', mode='r')\n            slc = get_images_slice(images.shape, width)\n            file_len = images.shape[0]\n            for i in range(0, file_len, batch):\n                image_chunck = images[i: i + batch if i + batch < file_len else file_len, slc, slc, channels]\n                info_chunck = info[i: i + batch if i + batch < file_len else file_len]\n                image_chunck = clean_images(image_chunck)\n                (train_image, _), _ = split_data(image_chunck, info_chunck)\n                files_data_len += train_image.shape[0]\n                for j in range(accumulators.shape[0]):\n                    accumulators[j] += np.sum((train_image[:, :, :, j] - mean[j]) ** 2)\n    stds = accumulators / (files_data_len * width * width)\n    stds = np.sqrt(stds)\n    return stds\n\nstd = get_std(ds, mean, batch=load_batch, width=img_w)\nprint(std)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef pre_process(files, width, means, stds, batch=1024):\n    # corta imagem grande o suficiente para poder rotacionar\n    rotation_width = int(np.ceil(np.sqrt((width ** 2) * 2)))\n    if rotation_width % 2 != 0:\n        rotation_width += 1\n\n    files_data_len = np.zeros(len(files))\n    with h5py.File(train_path, 'w') as train, h5py.File(valid_path, 'w') as valid:\n        for fi, file in enumerate(files):\n            with h5py.File(file, mode='r') as src:\n                images = src['matrix']\n                info = pd.read_hdf(file, key='info', mode='r')[['Vmax', 'time']]\n                slc = get_images_slice(images.shape, rotation_width)\n                file_len = images.shape[0]\n                for i in range(0, file_len, batch):\n                    batch_slc = slice(i, i + batch if i + batch < file_len else file_len)\n                    img_chunck = images[batch_slc, slc, slc, channels]\n                    info_chunck = info[batch_slc]\n                    img_chunck = clean_images(img_chunck)\n                    for j, (m, s) in enumerate(zip(means, stds)):\n                        img_chunck[:, :, :, j] -= m\n                        img_chunck[:, :, :, j] /= s\n\n                    img_new_shape = img_chunck.shape[1:]\n\n                    (train_img, train_info), (valid_img, valid_info) = split_data(img_chunck, info_chunck)\n\n                    if train_img.shape[0] > 0:\n                        if 'matrix' not in train:\n                            train.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n                        train['matrix'].resize(train['matrix'].shape[0] + train_img.shape[0], axis=0)\n                        train['matrix'][-train_img.shape[0]:] = train_img\n                        if 'info' not in train:\n                            train.create_dataset('info', shape=(0,), maxshape=(None,))\n                        train['info'].resize(train['info'].shape[0] + train_info.shape[0], axis=0)\n                        train['info'][-train_info.shape[0]:] = train_info\n\n\n                    if valid_img.shape[0] > 0:\n                        if 'matrix' not in valid:\n                            valid.create_dataset('matrix', shape=(0,) + img_new_shape, maxshape=(None,) + img_new_shape)\n                        valid['matrix'].resize(valid['matrix'].shape[0] + valid_img.shape[0], axis=0)\n                        valid['matrix'][-valid_img.shape[0]:] = valid_img\n                        if 'info' not in valid:\n                            valid.create_dataset('info', shape=(0,), maxshape=(None,))\n                        valid['info'].resize(valid['info'].shape[0] + valid_info.shape[0], axis=0)\n                        valid['info'][-valid_info.shape[0]:] = valid_info\n\n\npre_process(ds, img_w, mean, std, batch=load_batch)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with h5py.File(train_path, mode='r') as trainsrc, h5py.File(valid_path, mode='r') as validsrc:\n    data_len = trainsrc['matrix'].shape[0]\n    valid_data_len = validsrc['matrix'].shape[0]\n\n    print('Dataset de treino: ', trainsrc['matrix'].shape)\n    print('Dataset de validação: ', validsrc['matrix'].shape)\n\niter_train = data_len // batch\niter_valid = valid_data_len // batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_example(image, label):\n    image = tf.cast(image, tf.float32)\n    image = preprocess_image_tf(image)\n    return image, label\n\ndef preprocess_image_tf(image):\n    angle_rad = tf.random.uniform([], 0, 2 * np.pi)\n    image_shape = tf.shape(image)[0:2]\n    cx = tf.cast(image_shape[1] / 2, tf.float32)\n    cy = tf.cast(image_shape[0] / 2, tf.float32)\n    cos_a = tf.math.cos(angle_rad)\n    sin_a = tf.math.sin(angle_rad)\n    transform = tf.stack([\n        cos_a, -sin_a, (1 - cos_a) * cx + sin_a * cy,\n        sin_a,  cos_a, (1 - cos_a) * cy - sin_a * cx,\n        0.0,    0.0\n    ])\n    transform = tf.reshape(transform, [8])\n    transform = tf.expand_dims(transform, 0)\n    image = tf.expand_dims(image, 0)\n    rotated = tf.raw_ops.ImageProjectiveTransformV3(\n        images=image,\n        transforms=transform,\n        output_shape=image_shape,\n        interpolation=\"BILINEAR\",\n        fill_mode=\"REFLECT\",\n        fill_value=0.0\n    )\n    rotated = tf.squeeze(rotated, 0)\n    return tf.image.resize_with_crop_or_pad(rotated, img_w, img_w)\n\ndef load_dataset(file, batch_size):\n    with h5py.File(file, 'r') as f:\n        images = f['matrix'][:]\n        labels = f['info'][:]\n\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(buffer_size=len(images))\n    dataset = dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_data(file):\n    with h5py.File(file, mode='r') as src:\n        images = src['matrix'][:]\n        info = src['info'][:]\n        images = cut_images(images, img_w)\n        return tf.constant(images), tf.constant(info)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = load_dataset(train_path, batch)\n# valid_ds = load_dataset(processed_valid, batch)\nvalid_ds = get_data(valid_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef build_model(input_shape, strides=(2, 2)):\n    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n    reg = keras.regularizers.L2(1e-5)\n    model = keras.models.Sequential()\n    model.add(keras.layers.Input(input_shape))\n    model.add(keras.layers.Conv2D(16, (4, 4), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    model.add(keras.layers.Conv2D(32, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    model.add(keras.layers.Conv2D(64, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    model.add(keras.layers.Conv2D(128, (3, 3), strides=strides, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    \n    model.add(keras.layers.Flatten())\n    \n    model.add(keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n    model.add(keras.layers.Dense(1, activation='linear', kernel_initializer=initializer, kernel_regularizer=reg, bias_initializer=initializer, bias_regularizer=reg))\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=.5e-4), loss='mse', metrics=['mse'])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def training_number():\n    counter = 0\n    while True:\n        yield counter\n        counter += 1\n\ntraining_n_gen = training_number()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_n = next(training_n_gen)\n\nmodel = build_model((img_w, img_w, len(channels)),)\nmodel.summary()\n\nbest_model_path = '{epoch:03d}-{val_loss:.2f}.keras'\ncallback = ModelCheckpoint(filepath=best_model_path,\n                           monitor='val_loss',\n                           verbose=0,\n                           save_best_only=True,\n                           mode='min')\n\nwith tf.device('/GPU:0'):\n    history = model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=epochs,\n        steps_per_epoch=iter_train,\n        validation_steps=iter_valid,\n        callbacks=[callback],\n    )\n\nmodel_info_dict = {\n    \"seed\": seed,\n    \"shape\": [\n        img_w,\n        img_w,\n        len(channels)\n    ],\n    \"channels\": channels,\n    \"dataset\": ds,\n    \"batch\": batch,\n    \"normparams\": [{\"mean\": mean[i], \"std\": std[i]} for i in range(len(channels))],\n    \"validmse\": list(history.history['val_loss']),\n    \"trainingmse\": list(history.history['loss'])\n}\n\njson_info = json.dumps(model_info_dict, indent=4)\nwith open(f'n{training_n}-model_info.json', 'w') as outfile:\n    outfile.write(json_info)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}